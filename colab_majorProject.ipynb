{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "majorProject.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvl4opazyDOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a9c070f-093e-4bdc-c6ca-eef01a3c97a0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/Audio')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/Audio\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z42K6l1dFWlN",
        "outputId": "754f38e5-3111-4e88-9a97-7bc1d00a4af6"
      },
      "source": [
        "!pip install torch==1.4.0\n",
        "!pip install torchaudio==0.4.0\n",
        "!pip install syft==0.2.6"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.4.0\n",
            "  Using cached https://files.pythonhosted.org/packages/1a/3b/fa92ece1e58a6a48ec598bab327f39d69808133e5b2fb33002ca754e381e/torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl\n",
            "\u001b[31mERROR: torchvision 0.8.2+cu101 has requirement torch==1.7.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: syft 0.3.0 has requirement torch>=1.5, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.8.0\n",
            "    Uninstalling torch-1.8.0:\n",
            "      Successfully uninstalled torch-1.8.0\n",
            "Successfully installed torch-1.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchaudio==0.4.0 in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
            "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio==0.4.0) (1.4.0)\n",
            "Collecting syft==0.2.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/29/3fd78b6cecc540ebb31e676d269787d4bf5b4b90e479474a4f5ec9744bc0/syft-0.2.6-py3-none-any.whl (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 9.2MB/s \n",
            "\u001b[?25hCollecting syft-proto~=0.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/32/88f77ae5e7f2d800c905a1bce147760b044132489343e4fd2c2506389a44/syft_proto-0.4.10-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.0MB/s \n",
            "\u001b[?25hCollecting Pillow~=6.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/3f/03375124676ab49ca6e6917c0f1f663afb8354d5d24e12f4fe4587a39ae2/Pillow-6.2.2-cp37-cp37m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 15.7MB/s \n",
            "\u001b[?25hCollecting requests~=2.22.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch~=1.4.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.6) (1.4.0)\n",
            "Collecting phe~=1.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/32/0e/568e97b014eb14e794a1258a341361e9da351dc6240c63b89e1541e3341c/phe-1.4.0.tar.gz\n",
            "Collecting torchvision~=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/32/cb0e4c43cd717da50258887b088471568990b5a749784c465a8a1962e021/torchvision-0.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 21.8MB/s \n",
            "\u001b[?25hCollecting lz4~=3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/38/dacc3cbb33a9ded9e2e57f48707e8842f1080997901578ebddaa0e031646/lz4-3.0.2-cp37-cp37m-manylinux2010_x86_64.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 53.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: websockets~=8.1.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.6) (8.1)\n",
            "Collecting websocket-client~=0.57.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 57.0MB/s \n",
            "\u001b[?25hCollecting psutil==5.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/b8/3512f0e93e0db23a71d82485ba256071ebef99b227351f0f5540f744af41/psutil-5.7.0.tar.gz (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 52.1MB/s \n",
            "\u001b[?25hCollecting tblib~=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0d/de/dca3e651ca62e59c08d324f4a51467fa4b8cbeaafb883b5e83720b4d4a47/tblib-1.6.0-py2.py3-none-any.whl\n",
            "Collecting flask-socketio~=4.2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/66/44/edc4715af85671b943c18ac8345d0207972284a0cd630126ff5251faa08b/Flask_SocketIO-4.2.1-py2.py3-none-any.whl\n",
            "Collecting tornado==4.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/7b/e29ab3d51c8df66922fea216e2bddfcb6430fb29620e5165b16a216e0d3c/tornado-4.5.3.tar.gz (484kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 47.7MB/s \n",
            "\u001b[?25hCollecting aiortc==0.9.28\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/c5/0c15e562c5ea1531e8c7db1bcd524e53619cc27a228f3f28d2ba55544d38/aiortc-0.9.28-cp37-cp37m-manylinux2010_x86_64.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 51.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack~=1.0.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.6) (1.0.2)\n",
            "Requirement already satisfied: Flask~=1.1.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.6) (1.1.2)\n",
            "Collecting numpy~=1.18.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/c6/58e517e8b1fb192725cfa23c01c2e60e4e6699314ee9684a1c5f5c9b27e1/numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy~=1.4.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.6) (1.4.1)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from syft-proto~=0.4.5->syft==0.2.6) (3.12.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests~=2.22.0->syft==0.2.6) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests~=2.22.0->syft==0.2.6) (2020.12.5)\n",
            "Collecting idna<2.9,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests~=2.22.0->syft==0.2.6) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision~=0.5.0->syft==0.2.6) (1.15.0)\n",
            "Collecting python-socketio>=4.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/76/071fed5787169c45f72e07b64d71c77761326d6b76471d9773fbff634ce7/python_socketio-5.0.4-py2.py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pylibsrtp>=0.5.6 in /usr/local/lib/python3.7/dist-packages (from aiortc==0.9.28->syft==0.2.6) (0.6.8)\n",
            "Requirement already satisfied: cryptography>=2.2 in /usr/local/lib/python3.7/dist-packages (from aiortc==0.9.28->syft==0.2.6) (3.4.6)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from aiortc==0.9.28->syft==0.2.6) (1.14.5)\n",
            "Requirement already satisfied: crc32c in /usr/local/lib/python3.7/dist-packages (from aiortc==0.9.28->syft==0.2.6) (2.2)\n",
            "Collecting aioice<0.7.0,>=0.6.17\n",
            "  Downloading https://files.pythonhosted.org/packages/8b/86/e3cdf660b67da7a9a7013253db5db7cf786a52296cb40078db1206177698/aioice-0.6.18-py3-none-any.whl\n",
            "Requirement already satisfied: av<9.0.0,>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from aiortc==0.9.28->syft==0.2.6) (8.0.3)\n",
            "Requirement already satisfied: pyee>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from aiortc==0.9.28->syft==0.2.6) (8.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.1->syft==0.2.6) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.1->syft==0.2.6) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.1->syft==0.2.6) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.1->syft==0.2.6) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.2->syft-proto~=0.4.5->syft==0.2.6) (54.0.0)\n",
            "Collecting bidict>=0.21.0\n",
            "  Downloading https://files.pythonhosted.org/packages/67/d4/eaf9242722bf991e0955380dd6168020cb15a71cc0d3cc2373f4911b1f1d/bidict-0.21.2-py2.py3-none-any.whl\n",
            "Collecting python-engineio>=4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/29/efdb97d117b0f6538b046b576e33c1cbd59b4d1c2959cf8d0f8089a8f1b4/python_engineio-4.0.0-py2.py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.0->aiortc==0.9.28->syft==0.2.6) (2.20)\n",
            "Requirement already satisfied: netifaces in /usr/local/lib/python3.7/dist-packages (from aioice<0.7.0,>=0.6.17->aiortc==0.9.28->syft==0.2.6) (0.10.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->Flask~=1.1.1->syft==0.2.6) (1.1.1)\n",
            "Building wheels for collected packages: phe, psutil, tornado\n",
            "  Building wheel for phe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for phe: filename=phe-1.4.0-py2.py3-none-any.whl size=37362 sha256=d6085d8228c14c7c4a4eb23d92bba7bbbc31e9b1e8167dacd42e7795b3feb978\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/dc/36/dcb6bf0f1b9907e7b710ace63e64d08e7022340909315fdea4\n",
            "  Building wheel for psutil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for psutil: filename=psutil-5.7.0-cp37-cp37m-linux_x86_64.whl size=276441 sha256=634134a5fb7b17cd79c37f3df6bb93a20e73fa1745d2856e0889433cdd982e82\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/69/b4/3200b95828d1f0ddb3cb5699083717f4fdbd9b4223d0644c57\n",
            "  Building wheel for tornado (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tornado: filename=tornado-4.5.3-cp37-cp37m-linux_x86_64.whl size=434013 sha256=fb080ad9becb490baaefa03dcb05d60ca42692c94a266c7b4d8d2956ddd4061c\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/bf/f4/b68fa69596986881b397b18ff2b9af5f8181233aadcc9f76fd\n",
            "Successfully built phe psutil tornado\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=5.1.0; python_version >= \"3.0\", but you'll have tornado 4.5.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: bokeh 2.1.1 has requirement tornado>=5.1, but you'll have tornado 4.5.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: syft-proto, Pillow, idna, requests, phe, numpy, torchvision, lz4, websocket-client, psutil, tblib, bidict, python-engineio, python-socketio, flask-socketio, tornado, aioice, aiortc, syft\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "  Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: torchvision 0.8.2+cu101\n",
            "    Uninstalling torchvision-0.8.2+cu101:\n",
            "      Successfully uninstalled torchvision-0.8.2+cu101\n",
            "  Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Found existing installation: tblib 1.7.0\n",
            "    Uninstalling tblib-1.7.0:\n",
            "      Successfully uninstalled tblib-1.7.0\n",
            "  Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Found existing installation: aioice 0.7.5\n",
            "    Uninstalling aioice-0.7.5:\n",
            "      Successfully uninstalled aioice-0.7.5\n",
            "  Found existing installation: aiortc 1.1.2\n",
            "    Uninstalling aiortc-1.1.2:\n",
            "      Successfully uninstalled aiortc-1.1.2\n",
            "  Found existing installation: syft 0.3.0\n",
            "    Uninstalling syft-0.3.0:\n",
            "      Successfully uninstalled syft-0.3.0\n",
            "Successfully installed Pillow-6.2.2 aioice-0.6.18 aiortc-0.9.28 bidict-0.21.2 flask-socketio-4.2.1 idna-2.8 lz4-3.0.2 numpy-1.18.5 phe-1.4.0 psutil-5.7.0 python-engineio-4.0.0 python-socketio-5.0.4 requests-2.22.0 syft-0.2.6 syft-proto-0.4.10 tblib-1.6.0 torchvision-0.5.0 tornado-4.5.3 websocket-client-0.57.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "numpy",
                  "torchvision",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PghMt9XFWse"
      },
      "source": [
        "from syft.frameworks.torch.fl.utils import federated_avg"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnPHbct-z2md"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmgQ0ep7FO0b",
        "outputId": "9da0169e-4c52-4f26-b9d2-fe934a2b75f6"
      },
      "source": [
        "%%time\n",
        "import torchaudio\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from difflib import SequenceMatcher\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.functional import F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from librosa.core import stft, magphase\n",
        "from torch.autograd import Variable\n",
        "## from data_load import CodeSwitchDataset\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 518 ms, sys: 88.7 ms, total: 607 ms\n",
            "Wall time: 1.12 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ghu8CAFYrRP1",
        "outputId": "5031a71b-b364-4487-b4e6-c084412f9d25"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62OEVVwGFUVE"
      },
      "source": [
        "class CodeSwitchDataset(Dataset):\n",
        "    def __init__(self, lang, client, mode = \"train\", shuffle=True):\n",
        "        self.mode = mode\n",
        "        # data path\n",
        "        self.lang = lang\n",
        "        if self.lang == \"Gujarati\":\n",
        "            self.max_len = 0\n",
        "        elif self.lang == \"Telugu\":\n",
        "            self.max_len = 529862\n",
        "        elif self.lang == 'Tamil':\n",
        "            self.max_len = 0\n",
        "        else:\n",
        "            raise Exception(\"Check Language\")\n",
        "        if self.mode == \"train\":\n",
        "            self.path = 'Audio/MyDrive/PartB_{}/Dev/'.format(self.lang)\n",
        "        elif self.mode == \"test\":\n",
        "            self.path = self.path = 'Audio/MyDrive/PartB_{}/Dev/'.format(self.lang)\n",
        "        else:\n",
        "            raise Exception(\"Incorrect mode\")\n",
        "        self.file_list = os.listdir(os.path.join(self.path, 'Audio'))\n",
        "        self.shuffle=shuffle\n",
        "        self.csv_file = pd.read_csv(self.path + 'samples_450_{}.tsv'.format(client), header=None, sep='\\t')\n",
        "        self.input_length = []\n",
        "        self.label_length = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.csv_file)\n",
        "\n",
        "    def pad(self, wav, trans, max_len):\n",
        "        orig_len = len(wav)\n",
        "        while len(wav) < max_len:\n",
        "            diff = max_len - len(wav)\n",
        "            ext = wav[:diff]\n",
        "            wav = np.append(wav, wav[:diff])\n",
        "            ratio = int(len(trans)*diff/len(wav))\n",
        "            trans +=trans[:ratio]\n",
        "        return wav, trans\n",
        "\n",
        "    def preprocess(self, wav, sr, trans):\n",
        "\n",
        "        out = stft(wav, win_length=int(sr*0.02), hop_length=int(sr*0.01))\n",
        "        text_transform = TextTransform()\n",
        "        trans = torch.Tensor(text_transform.text_to_int(trans.lower()))\n",
        "\n",
        "        out = magphase(out)[0]\n",
        "        out = [np.log(1 + x) for x in out]\n",
        "        return np.array(out), trans\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_name = self.csv_file[0][idx]\n",
        "        trans = self.csv_file[1][idx]\n",
        "        wav, sr = librosa.load(glob(self.path + 'Audio/*'+ str(file_name) + '.wav')[0])\n",
        "\n",
        "        if len(set(trans)) > 2:\n",
        "            label = 1\n",
        "        elif len(set(trans)) == 1 or len(set(trans)) == 2:\n",
        "            label = 0\n",
        "        else:\n",
        "            raise Exception(\"Check transcript\")\n",
        "        if self.mode ==\"train\":\n",
        "            return wav, sr, trans, self.lang\n",
        "        elif self.mode == \"test\":\n",
        "            return wav\n",
        "        else:\n",
        "            raise Exception(\"Incorrect Mode\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_Hzcy_uI5GM"
      },
      "source": [
        "class TextTransform:\n",
        "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
        "    def __init__(self):\n",
        "        char_map_str = \"\"\"\n",
        "        s 1\n",
        "        e 2\n",
        "        t 3\n",
        "        \"\"\"\n",
        "        self.char_map = {}\n",
        "        self.index_map = {}\n",
        "        for line in char_map_str.strip().split('\\n'):\n",
        "            ch, index = line.split()\n",
        "            self.char_map[ch] = int(index)\n",
        "            self.index_map[int(index)] = ch\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
        "        int_sequence = []\n",
        "        for c in text:\n",
        "            if c == ' ':\n",
        "                ch = self.char_map['']\n",
        "            else:\n",
        "                ch = self.char_map[c]\n",
        "            int_sequence.append(ch)\n",
        "        return int_sequence\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
        "        string = []\n",
        "        for i in labels:\n",
        "            string.append(self.index_map[i])\n",
        "        return ''.join(string)\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03YASWhEI6d7"
      },
      "source": [
        "train_audio_transforms = nn.Sequential(\n",
        "            torchaudio.transforms.MelSpectrogram(n_mels=128, sample_rate = 22050, n_fft = 512, win_length=int(22050*0.02), hop_length=int(22050*0.01)),\n",
        "            torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
        "            torchaudio.transforms.TimeMasking(time_mask_param=35)\n",
        "        )\n",
        "\n",
        "\n",
        "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
        "\n",
        "text_transform = TextTransform()\n",
        "\n",
        "\n",
        "def data_processing(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, _, utterance, _) in data:\n",
        "        waveform=torch.Tensor(waveform)\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        spectrograms.append(spec)\n",
        "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0]//2)\n",
        "        label_lengths.append(len(label))\n",
        "\n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, padding_value = 1, batch_first=True)\n",
        "\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n",
        "\n",
        "\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "def wer(r, h):\n",
        "    \"\"\"\n",
        "    Calculation of WER with Levenshtein distance.\n",
        "\n",
        "    Works only for iterables up to 254 elements (uint8).\n",
        "    O(nm) time ans space complexity.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    r : list\n",
        "    h : list\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    int\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> wer(\"who is there\".split(), \"is there\".split())\n",
        "    1\n",
        "    >>> wer(\"who is there\".split(), \"\".split())\n",
        "    3\n",
        "    >>> wer(\"\".split(), \"who is there\".split())\n",
        "    3\n",
        "    \"\"\"\n",
        "    # initialisation\n",
        "\n",
        "    d = np.zeros((len(r) + 1) * (len(h) + 1), dtype=np.uint8)\n",
        "    d = d.reshape((len(r) + 1, len(h) + 1))\n",
        "    for i in range(len(r) + 1):\n",
        "        for j in range(len(h) + 1):\n",
        "            if i == 0:\n",
        "                d[0][j] = j\n",
        "            elif j == 0:\n",
        "                d[i][0] = i\n",
        "    for i in range(1, len(r) + 1):\n",
        "        for j in range(1, len(h) + 1):\n",
        "            if r[i - 1] == h[j - 1]:\n",
        "                d[i][j] = d[i - 1][j - 1]\n",
        "            else:\n",
        "                substitution = d[i - 1][j - 1] + 1\n",
        "                insertion = d[i][j - 1] + 1\n",
        "                deletion = d[i - 1][j] + 1\n",
        "                d[i][j] = min(substitution, insertion, deletion)\n",
        "    return d[len(r)][len(h)]\n",
        "\n",
        "def GreedyDecoder(output, labels, label_lengths, blank_label=0, collapse_repeated=True):\n",
        "    arg_maxes = torch.argmax(output, dim=2)\n",
        "    decodes = []\n",
        "    targets = []\n",
        "    for i, args in enumerate(arg_maxes):\n",
        "        decode = []\n",
        "        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "        for j, index in enumerate(args):\n",
        "            if index != blank_label:\n",
        "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
        "                    continue\n",
        "                decode.append(index.item())\n",
        "        decodes.append(text_transform.int_to_text(decode))\n",
        "    return decodes, targets"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7T7ZVbXI7tB"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ayRUTURI9qf"
      },
      "source": [
        "class CNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x (batch, channel, feature, time)\n",
        "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
        "\n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
        "        except with layer norm instead of batch norm\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x # (batch, channel, feature, time)\n",
        "\n",
        "\n",
        "class BidirectionalGRU(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "    \"\"\"Speech Recognition Model Inspired by DeepSpeech 2\"\"\"\n",
        "\n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        n_feats = n_feats//2\n",
        "        self.cnn = nn.Conv2d(1, 64, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
        "\n",
        "        # n residual cnn layers with filter size of 32\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "\n",
        "            ResidualCNN(64, 64, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = nn.Linear(n_feats*64, rnn_dim)\n",
        "        self.birnn_layers = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = self.rescnn_layers(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2) # (batch, time, feature)\n",
        "        x = self.fully_connected(x)\n",
        "        x = self.birnn_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik8ElvGiI-AY"
      },
      "source": [
        "def test(model, device, _data, criterion, epoch, writer, client_no):\n",
        "    model.eval()\n",
        "    training_loss, train_acc = 0, 0\n",
        "    eer, total_eer = 0, 0\n",
        "    test_loss=0\n",
        "    acc = []\n",
        "    with torch.no_grad():\n",
        "        inputs, labels, input_lengths, label_lengths = _data \n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # output = model(inputs, input_lengths)  # (batch, time, n_class)\n",
        "        output=model(inputs)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "\n",
        "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "        test_loss += loss.item() / len(test_loader)\n",
        "        decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "        decoded_preds, decoded_targets = list(map(str.strip, decoded_preds)), list(map(str.strip, decoded_targets))\n",
        "        for j in range(len(decoded_preds)):\n",
        "            s = SequenceMatcher(None, decoded_targets[j], decoded_preds[j])\n",
        "            acc.append(s.ratio())\n",
        "\n",
        "        avg_acc = sum(acc)/len(acc)\n",
        "        writer.add_scalar(\"{}/test_accuracy\".format(client_no), avg_acc, epoch)\n",
        "        writer.add_scalar(\"{}/WER\".format(client_no), wer(decoded_targets[j], decoded_preds[j]), iter_meter.get())\n",
        "        writer.add_scalar('{}/test_loss'.format(client_no), test_loss, epoch)\n",
        "        print(\"Test Accuracy: {}, Test loss: {}\".format(avg_acc, test_loss))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_OSoOpyI_by"
      },
      "source": [
        "def load_checkpoint(model, optimizer, filename='checkpoint.pth.tar'):\n",
        "    # Note: Input model & optimizer should be pre-defined.  This routine only updates their states.\n",
        "    start_epoch = 1\n",
        "    if os.path.isfile(filename):\n",
        "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
        "        checkpoint = torch.load(filename)\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
        "                  .format(filename, checkpoint['epoch']))\n",
        "    else:\n",
        "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
        "\n",
        "    return model, optimizer, start_epoch\n",
        "\n",
        "\n",
        "class IterMeter(object):\n",
        "    \"\"\"keeps track of total iterations\"\"\"\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.val += 1\n",
        "\n",
        "    def get(self):\n",
        "        return self.val"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ontTSp-pJAgK"
      },
      "source": [
        "device = 'cuda'\n",
        "\n",
        "hparams = {\n",
        "        \"n_cnn_layers\": 4,\n",
        "        \"n_rnn_layers\": 5,\n",
        "        \"rnn_dim\": 512,\n",
        "        \"n_class\": 4,\n",
        "        \"n_feats\": 128,\n",
        "        \"stride\": 2,\n",
        "        \"dropout\": 0.1,\n",
        "        \"learning_rate\": 5e-4,\n",
        "        \"batch_size\": 4,\n",
        "        \"epochs\": 60\n",
        "    }"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnWwgR2SJD3z"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAfkxa2TJEcD"
      },
      "source": [
        "train_dataset_client1 = CodeSwitchDataset(lang='Telugu', client = 'a', mode=\"train\")\n",
        "train_dataset_client2 = CodeSwitchDataset(lang='Telugu', client = 'b', mode=\"train\")\n",
        "validation_split = 0.2\n",
        "shuffle_dataset = True\n",
        "random_seed = 42\n",
        "dataset_size = len(train_dataset_client1)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "if shuffle_dataset:\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Creating PT data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "client1_train_loader = DataLoader(train_dataset_client1,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          drop_last=True,\n",
        "                          num_workers = 6,\n",
        "                          sampler = train_sampler,\n",
        "                          collate_fn = lambda x: data_processing(x, 'train'))\n",
        "\n",
        "\n",
        "client2_train_loader = DataLoader(train_dataset_client2,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          drop_last=True,\n",
        "                          num_workers = 6,\n",
        "                          sampler = train_sampler,\n",
        "                          collate_fn = lambda x: data_processing(x, 'train'))\n",
        "\n",
        "\n",
        "\n",
        "test_loader = DataLoader(train_dataset_client1,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          drop_last=True,\n",
        "                          num_workers=6,\n",
        "                          sampler=valid_sampler,\n",
        "                          collate_fn=lambda x: data_processing(x, 'valid'))\n",
        "\n",
        "\n",
        "device = torch.device('cuda')\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIRfb_ylJIFM"
      },
      "source": [
        "client1_model = SpeechRecognitionModel(\n",
        "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
        "        ).to(torch.device('cuda'))\n",
        "client1_optimizer = optim.Adam(client1_model.parameters(), hparams['learning_rate'])\n",
        "\n",
        "client2_model = SpeechRecognitionModel(\n",
        "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
        "        ).to(device)\n",
        "client2_optimizer = optim.Adam(client2_model.parameters(), hparams['learning_rate'])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1wyTzU4JKEV",
        "outputId": "0c57083d-fc0e-4fb7-9890-1a028e36db1f"
      },
      "source": [
        "criterion = nn.CTCLoss(blank=0, reduction='mean').to(device)\n",
        "epochs = 60\n",
        "epoch_num = 1\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(client1_optimizer,\n",
        "    max_lr=hparams['learning_rate'],\n",
        "    steps_per_epoch=int(len(client1_train_loader)),\n",
        "    epochs=hparams['epochs'],\n",
        "    anneal_strategy='linear')\n",
        "\n",
        "\n",
        "#model, optimizer, epoch_num = load_checkpoint(model, optimizer, \"checkpoints/ckpt_epoch_30_batch_id_1645.pth\")\n",
        "\n",
        "print(epoch_num)\n",
        "writer = SummaryWriter('train_logs_450_a_avg/')\n",
        "iter_meter = IterMeter()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRt2L-1QJK7o"
      },
      "source": [
        "def train(model, device, _data, criterion, optimizer, epoch, iter_meter, writer, scheduler, client_no):\n",
        "    model.train()\n",
        "    total_loss=0\n",
        "    LR = 0\n",
        "    train_loss = 0\n",
        "\n",
        "    avg_acc = 0\n",
        "    acc = []\n",
        "    wers = []\n",
        "    #bi, wav, label = batch_idx, wav, label\n",
        "    for g in optimizer.param_groups:\n",
        "        LR=g['lr']\n",
        "    wav, labels, input_lengths, label_lengths = _data\n",
        "    wav, labels = wav.to(device), labels.to(device)\n",
        "    # input_lengths, label_lengths = torch.IntTensor(input_lengths), torch.IntTensor(label_lengths)\n",
        "    wav = wav.to(device)\n",
        "    # wav = wav.float()\n",
        "    labels = labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    # output = model(wav, input_lengths)   #(batch, time, n_class) [4, 911, 3]\n",
        "    output = model(wav)\n",
        "\n",
        "    output = F.log_softmax(output, dim=2)\n",
        "    output = output.transpose(0,1)\n",
        "\n",
        "    # print(labels, label_lengths)\n",
        "    loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    #print(loss)\n",
        "    total_loss+=loss\n",
        "\n",
        "    iter_meter.step()\n",
        "    decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "    decoded_preds, decoded_targets = list(map(str.strip, decoded_preds)), list(map(str.strip, decoded_targets))\n",
        "    print(decoded_preds, decoded_targets)\n",
        "    print(\"preds: \", \"\".join(decoded_preds))\n",
        "    for j in range(len(decoded_preds)):\n",
        "        s = SequenceMatcher(None, decoded_targets[j], decoded_preds[j])\n",
        "        wers.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "        acc.append(s.ratio())\n",
        "\n",
        "    avg_acc = sum(acc)/len(acc)\n",
        "    writer.add_scalar(\"{}/accuracy/train_accuracy\".format(client_no), avg_acc, epoch)\n",
        "    writer.add_scalar('{}/accuracy/train_loss'.format(client_no), loss.item(), iter_meter.get())\n",
        "    writer.add_scalar('{}/CTCLoss'.format(client_no), loss, iter_meter.get())\n",
        "    writer.add_scalar('{}/TLoss'.format(client_no), total_loss, iter_meter.get())\n",
        "    writer.add_scalar(\"{}/Learning Rate\".format(client_no), LR, epoch)\n",
        "\n",
        "    writer.add_scalar(\"WER\", wer(decoded_targets[j], decoded_preds[j]), iter_meter.get())\n",
        "    \"\"\"if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(wav), data_len,\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "        print(\"Train Accuracy: {}, Train loss: {}\".format(avg_acc, train_loss))\n",
        "    \"\"\"\n",
        "\n",
        "    #print(decoded_preds[0])\n",
        "    if (epoch+1)%2 == 0:\n",
        "        model.eval().cpu()\n",
        "        ckpt_model_filename = \"{}_ckpt_epoch_\".format(client_no) + str(epoch+1) + \"_450a.pth\"\n",
        "        ckpt_model_path = os.path.join(\"Audio/MyDrive/checkpoints_avg/\", ckpt_model_filename)\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            }, ckpt_model_path)\n",
        "        model.to(device).train()\n",
        "    return model"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GFNOxreJWw_",
        "outputId": "0ff4a374-4d60-4cc5-f823-5dff28d66e98"
      },
      "source": [
        "models = [client1_model, client2_model]\n",
        "optimizers = [client1_optimizer, client2_optimizer]\n",
        "train_loaders = [client1_train_loader, client2_train_loader]\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    _test_data = next(iter(test_loader))\n",
        "    for i in range(len(models)):\n",
        "        _train_data = next(iter(train_loaders[i]))\n",
        "        models[i].to(device)\n",
        "        models[i] = train(models[i], device, _train_data, criterion, optimizers[i], epoch, iter_meter, writer, scheduler, \"client_{}\".format(i))\n",
        "        test(models[i], device, _test_data, criterion, epoch, writer, \"client_{}\".format(i))\n",
        "    fed_model = federated_avg({'client1': models[0],\n",
        "                              'client2': models[1]})\n",
        "    test(fed_model, device, _test_data, criterion, epoch, writer, \"client_{}\".format(i))\n",
        "fed_model.eval().cpu()\n",
        "save_model_filename = \"final_epoch_client1\" + str(epoch + 1)  + \".model\"\n",
        "save_model_path = os.path.join(\"fed_checkpoints\", save_model_filename)\n",
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': fed_model.state_dict(),\n",
        "            }, save_model_path)\n",
        "\n",
        "print(\"\\nDone, trained model saved at\", save_model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', '', '', ''] ['sstteeettttttttttttssstttttttttttttteeeettettttss', 'ssteeettttttttttteeeeeettsssstttsssssssseeseeetttttss', 'ssstttttttttttttsttttttttssstteettttttttttttttttts', 'ssstttttttttttssseeettttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.29831972989169037\n",
            "['teteettttetetteetetteetettetteteetetteeteetettteettteeeetttteetetetteteeeeteteteeeetteeeeeteetettetettetetteeeteteeetteetetteteteteetteteeeeteeeettet', 'tteteeetttttteteeeteeeteeeeeeeeeeteteeeeetteeeeeeeteeetteeeetetteeetetetteeeeeeeteteeeeeeteteeeeeeeteeeeeeteeete', 'teeeeteeeteeettteeteeeeteteeeeeteteeeeetetteteteeeeeteeeeeeeeeeeeteeeteeeeeeeteeeeeteeteteeeeteeeteteeeeeteteeete', 'eeeeeeeeeeeeeeeeeteeeeeeeeetteeeeeeeeeeeeeeeeteteeteeeeeeeeeeeeeeeeeeeeeetseeteeeeeeeeeeeeeeeeeeeeeeeeeeeeee'] ['ssssteeeeeetttttttttttss', 'ssstttttettttttteetteetttteststtttttttttttts', 'sssttttteessstttttsttttttttttss', 'seeeeeetttttttttttseetttttttttttttttsss']\n",
            "preds:  teteettttetetteetetteetettetteteetetteeteetettteettteeeetttteetetetteteeeeteteteeeetteeeeeteetettetettetetteeeteteeetteetetteteteteetteteeeeteeeettettteteeetttttteteeeteeeteeeeeeeeeeteteeeeetteeeeeeeteeetteeeetetteeetetetteeeeeeeteteeeeeeteteeeeeeeteeeeeeteeeteteeeeteeeteeettteeteeeeteteeeeeteteeeeetetteteteeeeeteeeeeeeeeeeeteeeteeeeeeeteeeeeteeteteeeeteeeteteeeeeteteeeteeeeeeeeeeeeeeeeeeteeeeeeeeetteeeeeeeeeeeeeeeeteteeteeeeeeeeeeeeeeeeeeeeeetseeteeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
            "Test Accuracy: 0.0, Test loss: 0.14849283478476785\n",
            "Test Accuracy: 0.0, Test loss: 0.419526143507524\n",
            "['', '', '', ''] ['sstteetttttttssttttttttss', 'sssttttttttseeettttstttttttttttttttttts', 'sssttttsstttttettttttss', 'sstttttttttttetttttttttttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.4156889481977983\n",
            "['', '', '', ''] ['ssseettteeeettetttteeettessettttttteetttttss', 'ssttttttttttseettttteeesseetttttttttttttttttttsetttss', 'ssttttteeetttttttttttttttttttss', 'sssssttteeesettssssssssssseetttsstttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.0503765344619751\n",
            "Test Accuracy: 0.0, Test loss: 0.1846019137989391\n",
            "['', '', '', ''] ['ssssssttttttttttttsstteeetttttttttttts', 'sstttettttttttttttttttttttss', 'ssseettttttttttttttttttttstttttttttttttttts', 'sstttttttttttttsssttttttttttsseeettttttstteeeeettttttttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.17989963834935968\n",
            "['', '', '', ''] ['sssttteettttttsssstttttttts', 'ssstteeeeettttstttttttttttss', 'sstttteeteetttttttttttttttsttttss', 'sseeeeeeeeesstttttttsssttttttttseeettttttttttsttttttttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.06789196621287953\n",
            "Test Accuracy: 0.0, Test loss: 0.05476254766637629\n",
            "['', '', '', ''] ['sstttttteeeetttteeeeeeeetttstttteeetteeettttts', 'ssttteeeeetteeettttttttttttss', 'sstteeeeeetttss', 'sstttttttttttttessettttttttttttsttttttttttttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.054626963355324486\n",
            "['', '', '', ''] ['sstttttttseettteetttsttttttttttttttss', 'sssseetttttetsssseettttttttss', 'sssttttetttsssttttttttttts', 'ssssstttttttteessssssssseeeeeetttssssssseeetesstttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.057353940877047455\n",
            "Test Accuracy: 0.0, Test loss: 0.048266806385733864\n",
            "['', '', '', ''] ['sssstettttttttttttstttttttttttttttttttttttttss', 'sssstttttttttttteeetts', 'sstteeettttttttttttsssssstttttts', 'ssttttttttstttteettstttteettttsttttssttttttttttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.048590595071965996\n",
            "['', '', '', ''] ['sssttttteettteeeeeeettttss', 'sstttttttttteettstttttttttttttttttsstttttttttttttttttttttts', 'sssttetttttttttttttss', 'ssttttttttttttseeeeseesssseeettttssseetttsssss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.04895007610321045\n",
            "Test Accuracy: 0.0, Test loss: 0.04688485102220015\n",
            "['', '', '', ''] ['ssssttttttttttttttssteettttttttttss', 'sstttttssttteetttttttttttttttttttss', 'ssseettttttts', 'ssttttttteeetttsssttttttttttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.04690514369444414\n",
            "['', '', '', ''] ['sssseetttttetsssseettttttttss', 'sssteeetttttsttttttttttttttttttttttss', 'sstttttttttttttttttttseeetttttts', 'ssssstttttttttttssstttttttttteetttttttttttttts']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.05923894860527732\n",
            "Test Accuracy: 0.0, Test loss: 0.04957062547857111\n",
            "['', '', '', ''] ['sseeeeeetttsttteettttttttstttttttss', 'sstttttttttttttttttstttttttteetttttttttttss', 'sstttttteettttteeetttttsss', 'sssttttsstttttettttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.041391933506185356\n",
            "['', '', '', ''] ['ssttttteeseettttttstttttttss', 'ssttttttttseeettttttttttttttss', 'sstttttttttttttteettttts', 'sstteettttttttttseeeeeseetteeeseessseettttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.040730427611957894\n",
            "Test Accuracy: 0.0, Test loss: 0.04125194928862832\n",
            "['', '', '', ''] ['sstttttttettttss', 'sseeesstttetsttttttttttttss', 'sstttttttttttttsttttttssttttssssssstttttttttteeeeeeeettttss', 'sstttttttttttttttttttttttteeetttttttttttttttttttttttssss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.05749239162965254\n",
            "['', '', '', ''] ['sseeeeeetttttteettttttttss', 'sssstttttsttttttttsstttss', 'ssseetttttteettttttttttts', 'ssssteettttttssttttttttsss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.05615378509868275\n",
            "Test Accuracy: 0.0, Test loss: 0.055548657070506706\n",
            "['', '', '', ''] ['sstttttettttttttttttss', 'sstttttssttteetttttttttttttttttttss', 'ssstttteeeesttttts', 'sssssssstttttteettttttstttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.03923260352828286\n",
            "['', '', '', ''] ['ssstttttttttttttttsttess', 'ssstttttttttteettssttttttteeeettts', 'ssseetttttesttttttss', 'ssstteeeeettttstttttttttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.0427640513940291\n",
            "Test Accuracy: 0.0, Test loss: 0.04030230099504644\n",
            "['', '', '', ''] ['sstttteetttttteetttttttttts', 'ssttttttttttttttttttttttttttttttttttsess', 'sssttttttttseeettttstttttttttttttttttts', 'ssssttttteesttttttttttttts']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.040870319713245735\n",
            "['', '', '', ''] ['ssseetttttttttttttts', 'sssttttttttteetttttttttttttttttts', 'sstttttteeetttttetttss', 'sssteeetttttsttttttttttttttttttttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.042244285345077515\n",
            "Test Accuracy: 0.0, Test loss: 0.04158561879938299\n",
            "['', '', '', ''] ['sstttttttttttttttttttttssssssseeetttttttttttttttttss', 'sstttttteetttttttttss', 'sseeeeetttttttttss', 'sssssssssttteeettsettttttttttttttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.04479860717600042\n",
            "['', '', '', ''] ['sssttttttttteetssttttttttttsss', 'sstttttttttttttttttttseeetttttts', 'seeetttttttttttttttttttettttteeettttttts', 'sstttttttttttteettttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.04312211275100708\n",
            "Test Accuracy: 0.0, Test loss: 0.04366468841379339\n",
            "['', '', '', ''] ['sssssssstttttetttttttstttsttttteessttttttttss', 'sstttttteeeestttttttttttssssttttttteettttttttts', 'ssstttteeeesttttts', 'sstttttttettttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.042498767375946045\n",
            "['', '', '', ''] ['ssstttttttttttettttttstttts', 'ssssttttttteessttttttttsstttttsssss', 'ssttttttttttteeeeeetttttttttttteeetttsttttttttttts', 'sssttttttttttttttttttttttttttttsssssttttttttttteetsssttttttttttttttttttsstttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.04304010488770225\n",
            "Test Accuracy: 0.0, Test loss: 0.04178902235898105\n",
            "['', '', '', ''] ['sstttteeetteeeeettsssss', 'sssstttttettttttttss', 'ssttttttttttttttttttttetttttssttttttttttttttttts', 'ssssssttttttttttttsstteeetttttttttttts']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.044080195101824676\n",
            "['', '', '', ''] ['sssstttttttttteettttttsttteeettttttttsttttttttttttttttss', 'ssstttttttttttttttettttttttss', 'ssttttttttttttttttttttttttttttettttts', 'sssssttttttttttttteettttttssssttttttttsssssssss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.04768346656452526\n",
            "Test Accuracy: 0.0, Test loss: 0.044959745623848656\n",
            "['', '', '', ''] ['ssttttttttteeetseeetttttttttsssstttttttttss', 'ssssttttteesttttttttttttts', 'sssseeetttttttttttss', 'ssssstttttttttttssssseeeesstttts']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.03954324939034202\n",
            "['', '', '', ''] ['sstttttttttttttteettttts', 'sssssssssssttttttettttttttttttttttttss', 'sseeeeeetttttteettttttttss', 'ssstttttttttttttttettttttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.041406084190715446\n",
            "Test Accuracy: 0.0, Test loss: 0.04015197266231884\n",
            "['', '', '', ''] ['ssttttttttttttsseeesseeettttttttts', 'ssssttttttttteettttttttttttetttttts', 'sstttttttttteetttss', 'ssssseeeettttttssttttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.038364884528246795\n",
            "['', '', '', ''] ['sssttttttttttttttttttteeettttttttttsttttts', 'sssttttteeettttttttttttttss', 'sssttttttttstttttttsttteettsttss', 'ssstttttteetttttttttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.03740793466567993\n",
            "Test Accuracy: 0.0, Test loss: 0.03774329207160256\n",
            "['', '', '', ''] ['ssttttttttttttsstttttssttteeeeetttssssstttttss', 'sseettsssetttttttteeessssttttttttsssttttttttttss', 'ssttttteettttttsstttttss', 'sseeeeeetttttssseeeeeetttttttttttttttttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.04825885187495838\n",
            "['', '', '', ''] ['ssstttttttetttttttttttttttts', 'sseeeeeeeeeettttttssstttts', 'ssstttttteettteeetttttsssstttttttss', 'ssstttttttttttttteeeetttttttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.047383817759427155\n",
            "Test Accuracy: 0.0, Test loss: 0.047507811676372184\n",
            "['', '', '', ''] ['sstteeeeetteettsssttttttttettttttss', 'ssttteettttetttttttttttttttttttttsttttts', 'sstttttttttttttttttttttsstttttttteettttttttttttttttss', 'ssttttttttettttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.04561417753046209\n",
            "['', '', '', ''] ['sseettttttteesssttttttssstttssstttttttttts', 'ssssteeeeeetttttttttttss', 'sseeettttttstttttttttttttss', 'sseeettssttttttssssssssstttttttttttttssssssssssssssssssssttttttttsssssttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.044865310192108154\n",
            "Test Accuracy: 0.0, Test loss: 0.04482496055689725\n",
            "['', '', '', ''] ['sstttttttttttttttttttttttstttttttttteettttttss', 'ssssttteetsttttteetttttttsss', 'ssttttsssssssseetttttttttttssssssssssssssssstttttts', 'sstttttttttttttttttttttttttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.04048843817277388\n",
            "['', '', '', ''] ['sssttssttttttttttttttetttttsss', 'ssstttttetttttttsttttss', 'sstttttttseettteetttsttttttttttttttss', 'ssstttetttttttttttttts']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.04111668196591464\n",
            "Test Accuracy: 0.0, Test loss: 0.040581334720958366\n",
            "['', '', '', ''] ['ssstttttttttttteetttttttttttttttt', 'sseeteeeeeeeeettetttttttttttttss', 'sstttteeeeeseeeetttsssstttssssstttttttttttttssssssssstttttttttssttttttttssssssssssstttttttttttttttttss', 'sstttttteeeettttttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.03668088262731379\n",
            "['', '', '', ''] ['ssssssttttsssseeeetstttttttttts', 'sstttttttttttttttttsssssseeeesssssttttttttttttttss', 'sssttteeeeeeeeeesssseetttttss', 'ssssssseeetttttttteettttttttttttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.039325844157825814\n",
            "Test Accuracy: 0.0, Test loss: 0.037692470984025436\n",
            "['', '', '', ''] ['ssttttttttttttsstttttssttteeeeetttssssstttttss', 'ssssttttttttttteeeeetttsttttttttttttttss', 'sstttttetttttttttttssss', 'ssttttttteettttttttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.04994724013588645\n",
            "['', '', '', ''] ['ssstttttettttttteetteetttteststtttttttttttts', 'sssstttttttttteettttttsttteeettttttttsttttttttttttttttss', 'ssttttttttttttteetttts', 'sseeettssttttttssssssssstttttttttttttssssssssssssssssssssttttttttsssssttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.04972031983462247\n",
            "Test Accuracy: 0.0, Test loss: 0.049289215694774284\n",
            "['', '', '', ''] ['sstteettttetttssstttttts', 'sseeettttttttttttsssssssstttttssttttttttss', 'sstttteeetteeeeettsssss', 'sstttttttettteettttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.04624862562526356\n",
            "['', '', '', ''] ['ssstteetssssttettttss', 'sstttttttteeesttttttttss', 'sssstttttttttsseeeeeseetttttteeettttss', 'sssttttsssssttttttttttssstttttsttttttttttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.05071586912328547\n",
            "Test Accuracy: 0.0, Test loss: 0.04795514453541149\n",
            "['', '', '', ''] ['ssstttttteetetstttttttttttttttss', 'sstttttttttttttsstttttsseetttts', 'ssttttttttssstttttteeetttttttteettttss', 'ssstttttttttteetttttttts']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.05346460234035145\n",
            "['', '', '', ''] ['sssssttttteetttttseeeeettttttttsttttttttttttttttttttstttttttttttttss', 'sstttttttttttstttttteeeeetttttts', 'sseeeetttttstttttttttttts', 'ssstttttttttttsssttteettttttttttttttttttss']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.0544709942557595\n",
            "Test Accuracy: 0.0, Test loss: 0.053838003765452995\n",
            "['', '', '', ''] ['ssteeettttttttttteeeeeettsssstttsssssssseeseeetttttss', 'sstttttttetseetttttts', 'ssseettttsssssttttttss', 'ssttttttttttttttttttttetttttssttttttttttttttttts']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.047801770947196266\n",
            "['', '', '', ''] ['ssttttttttttseettttteeesseetttttttttttttttttttsetttss', 'sseeettttttstttttttttttttss', 'sssssttttteetttttseeeeettttttttsttttttttttttttttttttstttttttttttttss', 'sstttettttttttttttttts']\n",
            "preds:  \n",
            "Test Accuracy: 0.0, Test loss: 0.046286615458401764\n",
            "Test Accuracy: 0.0, Test loss: 0.046969343315471306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nuTdMLx-JoNx",
        "outputId": "8a6ba845-4169-4306-e2fa-419210e754d2"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGcK__OBt3Mu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}