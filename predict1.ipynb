{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cTfYifImtoSJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dexter/Desktop/MSR challenge/hparam.py:11: YAMLLoadWarning: calling yaml.load_all() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  for doc in docs:\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import csv\n",
    "from data_load import TextTransform\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#from decoder import GreedyDecoder\n",
    "from torch.functional import F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "from librosa.core import stft, magphase\n",
    "from glob import glob\n",
    "from torch import autograd\n",
    "import csv\n",
    "from data_load import CodeSwitchDataset\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dexter/Desktop/Projects/CodeSwitching/hparam.py:11: YAMLLoadWarning: calling yaml.load_all() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  for doc in docs:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torchaudio\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from difflib import SequenceMatcher\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.functional import F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "from librosa.core import stft, magphase\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "from data_load import CodeSwitchDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransform:\n",
    "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
    "    def __init__(self):\n",
    "        char_map_str = \"\"\"\n",
    "        s 1\n",
    "        e 2\n",
    "        t 3\n",
    "        \"\"\"\n",
    "        self.char_map = {}\n",
    "        self.index_map = {}\n",
    "        for line in char_map_str.strip().split('\\n'):\n",
    "            ch, index = line.split()\n",
    "            self.char_map[ch] = int(index)\n",
    "            self.index_map[int(index)] = ch\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
    "        int_sequence = []\n",
    "        for c in text:\n",
    "            if c == ' ':\n",
    "                ch = self.char_map['']\n",
    "            else:\n",
    "                ch = self.char_map[c]\n",
    "            int_sequence.append(ch)\n",
    "        return int_sequence\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
    "        string = []\n",
    "        for i in labels:\n",
    "            string.append(self.index_map[i])\n",
    "        return ''.join(string)\n",
    "\n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "            torchaudio.transforms.MelSpectrogram(n_mels=128, sample_rate = 22050, n_fft = 512, win_length=int(22050*0.02), hop_length=int(22050*0.01)),\n",
    "            torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
    "            torchaudio.transforms.TimeMasking(time_mask_param=35)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeSwitchDataset(Dataset):\n",
    "    def __init__(self, lang, mode = \"train\", shuffle=True):\n",
    "        self.mode = mode\n",
    "        # data path\n",
    "        self.lang = lang\n",
    "        if self.lang == \"Gujarati\":\n",
    "            self.max_len = 0\n",
    "        elif self.lang == \"Telugu\":\n",
    "            self.max_len = 529862\n",
    "        elif self.lang == 'Tamil':\n",
    "            self.max_len = 0\n",
    "        else:\n",
    "            raise Exception(\"Check Language\")\n",
    "        if self.mode == \"train\":\n",
    "            self.path = 'Data/PartB_{}/Dev/'.format(self.lang)\n",
    "        elif self.mode == \"test\":\n",
    "            self.path = self.path = 'Data/PartB_{}/Dev/'.format(self.lang)\n",
    "        else:\n",
    "            raise Exception(\"Incorrect mode\")\n",
    "        self.file_list = os.listdir(os.path.join(self.path, 'Audio'))\n",
    "        self.shuffle=shuffle\n",
    "        self.csv_file = pd.read_csv(self.path + 'dev.tsv', header=None, sep='\\t')\n",
    "        self.input_length = []\n",
    "        self.label_length = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_file)\n",
    "\n",
    "    def pad(self, wav, trans, max_len):\n",
    "        orig_len = len(wav)\n",
    "        while len(wav) < max_len:\n",
    "            diff = max_len - len(wav)\n",
    "            ext = wav[:diff]\n",
    "            wav = np.append(wav, wav[:diff])\n",
    "            ratio = int(len(trans)*diff/len(wav))\n",
    "            trans +=trans[:ratio]\n",
    "        return wav, trans\n",
    "\n",
    "    def preprocess(self, wav, sr, trans):\n",
    "\n",
    "        out = stft(wav, win_length=int(sr*0.02), hop_length=int(sr*0.01))\n",
    "        text_transform = TextTransform()\n",
    "        trans = torch.Tensor(text_transform.text_to_int(trans.lower()))\n",
    "\n",
    "        out = magphase(out)[0]\n",
    "        out = [np.log(1 + x) for x in out]\n",
    "        return np.array(out), trans\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.csv_file[0][idx]\n",
    "        trans = self.csv_file[1][idx]\n",
    "        wav, sr = librosa.load(glob(self.path + 'Audio/*'+ str(file_name) + '.wav')[0])\n",
    "\n",
    "        if len(set(trans)) > 2:\n",
    "            label = 1\n",
    "        elif len(set(trans)) == 1 or len(set(trans)) == 2:\n",
    "            label = 0\n",
    "        else:\n",
    "            raise Exception(\"Check transcript\")\n",
    "        if self.mode ==\"train\":\n",
    "            return wav, sr, trans, self.lang\n",
    "        elif self.mode == \"test\":\n",
    "            return wav\n",
    "        else:\n",
    "            raise Exception(\"Incorrect Mode\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
    "\n",
    "text_transform = TextTransform()\n",
    "\n",
    "\n",
    "def data_processing(data, data_type=\"train\"):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    for (waveform, _, utterance, _) in data:\n",
    "        waveform=torch.Tensor(waveform)\n",
    "        if data_type == 'train':\n",
    "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        spectrograms.append(spec)\n",
    "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0]//2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, padding_value = 1, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GreedyDecoder(output, labels, label_lengths, blank_label=0, collapse_repeated=True):\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    targets = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank_label:\n",
    "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
    "                    continue\n",
    "                decode.append(index.item())\n",
    "        decodes.append(text_transform.int_to_text(decode))\n",
    "    return decodes, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer(r, h):\n",
    "    \"\"\"\n",
    "    Calculation of WER with Levenshtein distance.\n",
    "\n",
    "    Works only for iterables up to 254 elements (uint8).\n",
    "    O(nm) time ans space complexity.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    r : list\n",
    "    h : list\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> wer(\"who is there\".split(), \"is there\".split())\n",
    "    1\n",
    "    >>> wer(\"who is there\".split(), \"\".split())\n",
    "    3\n",
    "    >>> wer(\"\".split(), \"who is there\".split())\n",
    "    3\n",
    "    \"\"\"\n",
    "    # initialisation\n",
    "    d = np.zeros((len(r) + 1) * (len(h) + 1), dtype=np.uint8)\n",
    "    d = d.reshape((len(r) + 1, len(h) + 1))\n",
    "    for i in range(len(r) + 1):\n",
    "        for j in range(len(h) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "    for i in range(1, len(r) + 1):\n",
    "        for j in range(1, len(h) + 1):\n",
    "            if r[i - 1] == h[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    return d[len(r)][len(h)]/len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CodeSwitchDataset(lang='Telugu', mode=\"train\")\n",
    "validation_split = 0.2\n",
    "shuffle_dataset = True\n",
    "BATCH_SIZE=4\n",
    "random_seed = 42\n",
    "dataset_size = len(train_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          drop_last=True,\n",
    "                          num_workers = 6,\n",
    "                          sampler = train_sampler,\n",
    "                          collate_fn = lambda x: data_processing(x, 'train'))\n",
    "\n",
    "test_loader = DataLoader(train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          drop_last=True,\n",
    "                          num_workers=6,\n",
    "                          sampler=valid_sampler,\n",
    "                          collate_fn=lambda x: data_processing(x, 'valid'))\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CNNLayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
    "    def __init__(self, n_feats):\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "    def forward(self, x):\n",
    "        # x (batch, channel, feature, time)\n",
    "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
    "\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
    "        except with layer norm instead of batch norm\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "    def forward(self, x):\n",
    "        residual = x  # (batch, channel, feature, time)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        return x # (batch, channel, feature, time)\n",
    "\n",
    "\n",
    "class BidirectionalGRU(nn.Module):\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "        self.BiGRU = nn.GRU(\n",
    "            input_size=rnn_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.gelu(x)\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    \"\"\"Speech Recognition Model Inspired by DeepSpeech 2\"\"\"\n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        n_feats = n_feats//2\n",
    "        self.cnn = nn.Conv2d(1, 64, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
    "        # n residual cnn layers with filter size of 32\n",
    "        self.rescnn_layers = nn.Sequential(*[\n",
    "            ResidualCNN(64, 64, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "        self.fully_connected = nn.Linear(n_feats*64, rnn_dim)\n",
    "        self.birnn_layers = nn.Sequential(*[\n",
    "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
    "            for i in range(n_rnn_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn_layers(x)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "        x = x.transpose(1, 2) # (batch, time, feature)\n",
    "        x = self.fully_connected(x)\n",
    "        x = self.birnn_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "data_len = len(train_loader.dataset)\n",
    "pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "for batch_idx, (_data) in pbar:\n",
    "    #bi, wav, label = batch_idx, wav, label\n",
    "    wav, labels, input_lengths, label_lengths = _data\n",
    "    print(wav.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, writer):\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    total_loss=0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for batch_idx, (_data) in pbar:\n",
    "        #bi, wav, label = batch_idx, wav, label\n",
    "        wav, labels, input_lengths, label_lengths = _data\n",
    "        wav = wav.to(device)\n",
    "        wav = wav.float()\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "        output, _ = model(wav)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        output = output.transpose(0,1)\n",
    "        #print(output.shape)\n",
    "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "        #print(loss)\n",
    "        total_loss+=loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        iter_meter.step()\n",
    "        \n",
    "        writer.add_scalar('Loss', loss, epoch*len(train_loader)+1)\n",
    "        writer.add_scalar('TLoss', total_loss, epoch*len(train_loader)+1)\n",
    "        if batch_idx % 100 == 0 or batch_idx == data_len:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(wav), data_len,\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "    if (epoch+1)%2 == 0:\n",
    "        model.eval().cpu()\n",
    "        ckpt_model_filename = \"ckpt_epoch_\" + str(epoch+1) + \"_batch_id_\" + str(batch_idx+1) + \".pth\"\n",
    "        ckpt_model_path = os.path.join(\"checkpoints/\", ckpt_model_filename)\n",
    "        torch.save(model.state_dict(), ckpt_model_path)\n",
    "        model.to(device).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion, epoch, writer):\n",
    "    model.eval()\n",
    "    training_loss, train_acc = 0, 0\n",
    "    eer, total_eer = 0, 0\n",
    "    test_loss=0\n",
    "    acc = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, _data in enumerate(test_loader):\n",
    "            inputs, labels, input_lengths, label_lengths = _data \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # output = model(inputs, input_lengths)  # (batch, time, n_class)\n",
    "            output=model(inputs)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            test_loss += loss.item() / len(test_loader)\n",
    "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "            decoded_preds, decoded_targets = list(map(str.strip, decoded_preds)), list(map(str.strip, decoded_targets))\n",
    "            for j in range(len(decoded_preds)):\n",
    "                s = SequenceMatcher(None, decoded_targets[j], decoded_preds[j])\n",
    "                acc.append(s.ratio())\n",
    "\n",
    "            avg_acc = sum(acc)/len(acc)\n",
    "            writer.add_scalar(\"test_accuracy\", avg_acc, epoch)\n",
    "            writer.add_scalar('test_loss', test_loss, epoch)\n",
    "            writer.add_scalar(\"WER\", wer(decoded_targets[j], decoded_preds[j]), iter_meter.get())\n",
    "            print(\"Test Accuracy: {}, Test loss: {}\".format(avg_acc, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, filename='checkpoint.pth.tar'):\n",
    "    # Note: Input model & optimizer should be pre-defined.  This routine only updates their states.\n",
    "    start_epoch = 1\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "        checkpoint = torch.load(filename)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(filename, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "\n",
    "    return model, optimizer, start_epoch\n",
    "\n",
    "\n",
    "class IterMeter(object):\n",
    "    \"\"\"keeps track of total iterations\"\"\"\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.val += 1\n",
    "\n",
    "    def get(self):\n",
    "        return self.val\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "hparams = {\n",
    "        \"n_cnn_layers\": 4,\n",
    "        \"n_rnn_layers\": 5,\n",
    "        \"rnn_dim\": 512,\n",
    "        \"n_class\": 4,\n",
    "        \"n_feats\": 128,\n",
    "        \"stride\": 2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"batch_size\": 4,\n",
    "        \"epochs\": 60\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = SpeechRecognitionModel(\n",
    "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.CTCLoss(blank=0, reduction='mean').to(device)\n",
    "epochs = 60\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), hparams['learning_rate'])\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "    max_lr=hparams['learning_rate'],\n",
    "    steps_per_epoch=int(len(train_loader)),\n",
    "    epochs=hparams['epochs'],\n",
    "    anneal_strategy='linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_logs(model, device, test_loader, criterion, epoch, optimizer, iter_meter):\n",
    "    model.eval()\n",
    "    training_loss, train_acc = 0, 0\n",
    "    eer, total_eer = 0, 0\n",
    "    test_loss=0\n",
    "    acc = []\n",
    "    cers = {}\n",
    "    ps = ['final_epoch_61.model', 'final_epoch_client161.model', 'final_epoch_client261.model']\n",
    "    for p in ps:\n",
    "        cers = []\n",
    "        model, optimizer, epoch_num = load_checkpoint(model, optimizer, \"checkpoints/{}\".format(p))\n",
    "        writer = SummaryWriter('testing_logs_2/{}'.format(p))\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, _data in enumerate(test_loader):\n",
    "                inputs, labels, input_lengths, label_lengths = _data \n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                # output = model(inputs, input_lengths)  # (batch, time, n_class)\n",
    "                output=model(inputs)\n",
    "                output = F.log_softmax(output, dim=2)\n",
    "                output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "                loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "                test_loss += loss.item() / len(test_loader)\n",
    "                decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "                decoded_preds, decoded_targets = list(map(str.strip, decoded_preds)), list(map(str.strip, decoded_targets))\n",
    "                for j in range(len(decoded_preds)):\n",
    "                    s = SequenceMatcher(None, decoded_targets[j], decoded_preds[j])\n",
    "                    acc.append(s.ratio())\n",
    "\n",
    "                avg_acc = sum(acc)/len(acc)\n",
    "                writer.add_scalar(\"test_accuracy\", avg_acc, iter_meter.get())\n",
    "                writer.add_scalar('test_loss', test_loss, iter_meter.get())\n",
    "                \n",
    "                cers.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "                \n",
    "                \n",
    "                writer.add_scalar(\"WERs/\", wer(decoded_targets[j], decoded_preds[j]), iter_meter.get())\n",
    "                print(\"Test Accuracy: {}, Test loss: {}\".format(avg_acc, test_loss))\n",
    "                iter_meter.step()\n",
    "                \n",
    "# model, optimizer, epoch_num = load_checkpoint(model, optimizer, \"checkpoints/final_epoch_61.model\")\n",
    "# print(epoch_num)\n",
    "writer = SummaryWriter('testing_logs_2/')\n",
    "iter_meter = IterMeter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'checkpoints/final_epoch_61.model'\n",
      "=> loaded checkpoint 'checkpoints/final_epoch_61.model' (epoch 60)\n",
      "Test Accuracy: 0.7444622697563874, Test loss: 0.13348866360528128\n",
      "Test Accuracy: 0.7375642799939476, Test loss: 0.3068878395216805\n",
      "Test Accuracy: 0.7855682756194265, Test loss: 0.414943482194628\n",
      "Test Accuracy: 0.7960804962081451, Test loss: 0.5394780550684247\n",
      "Test Accuracy: 0.782076431927128, Test loss: 0.8686799917902265\n",
      "Test Accuracy: 0.7719812087860126, Test loss: 1.020795064313071\n",
      "Test Accuracy: 0.7727378971955595, Test loss: 1.3154792700495037\n",
      "=> loading checkpoint 'checkpoints/final_epoch_client161.model'\n",
      "=> loaded checkpoint 'checkpoints/final_epoch_client161.model' (epoch 60)\n",
      "Test Accuracy: 0.7707650209483701, Test loss: 1.6215578062193732\n",
      "Test Accuracy: 0.7722896364631822, Test loss: 1.8771128228732517\n",
      "Test Accuracy: 0.7846053245460551, Test loss: 1.955582627228328\n",
      "Test Accuracy: 0.7630614303872284, Test loss: 2.1419659086636136\n",
      "Test Accuracy: 0.7695251709751502, Test loss: 2.324731409549713\n",
      "Test Accuracy: 0.7722571852205313, Test loss: 2.4942477686064586\n",
      "Test Accuracy: 0.7680817588606483, Test loss: 2.7102577601160323\n",
      "=> loading checkpoint 'checkpoints/final_epoch_client261.model'\n",
      "=> loaded checkpoint 'checkpoints/final_epoch_client261.model' (epoch 60)\n",
      "Test Accuracy: 0.7729505032170916, Test loss: 2.8028524688311984\n",
      "Test Accuracy: 0.7780346009068101, Test loss: 2.9206409539495195\n",
      "Test Accuracy: 0.7726120373387623, Test loss: 3.1639677711895535\n",
      "Test Accuracy: 0.7659607811525673, Test loss: 3.4080797178404674\n",
      "Test Accuracy: 0.7617447015452685, Test loss: 3.5792825307164877\n",
      "Test Accuracy: 0.7643126222441285, Test loss: 3.7887713653700694\n",
      "Test Accuracy: 0.7621766757447416, Test loss: 3.9471014652933394\n",
      "=> loading checkpoint 'checkpoints/final_epoch_61.model'\n",
      "=> loaded checkpoint 'checkpoints/final_epoch_61.model' (epoch 60)\n",
      "Test Accuracy: 0.7110781730346947, Test loss: 0.13846487658364431\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-da5b7135bcbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#train(model, device, train_loader, criterion, optimizer, epoch, iter_meter, writer, scheduler)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtest_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_meter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-7b55646dc5f6>\u001b[0m in \u001b[0;36mtest_logs\u001b[0;34m(model, device, test_loader, criterion, epoch, optimizer, iter_meter)\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mdecoded_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGreedyDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0mdecoded_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c3ec13ba2153>\u001b[0m in \u001b[0;36mGreedyDecoder\u001b[0;34m(output, labels, label_lengths, blank_label, collapse_repeated)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdecode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_to_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlabel_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mblank_label\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollapse_repeated\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m    460\u001b[0m                           \u001b[0;34m'iterations executed (and might lead to errors or silently give '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                           'incorrect results).', category=RuntimeWarning)\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    #train(model, device, train_loader, criterion, optimizer, epoch, iter_meter, writer, scheduler)\n",
    "    test_logs(model, device, test_loader, criterion, epoch, optimizer, iter_meter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'checkpoints/final_epoch_61.model'\n",
      "=> loaded checkpoint 'checkpoints/final_epoch_61.model' (epoch 60)\n",
      "cers:  0.4523809523809524\n",
      "cers:  0.48\n",
      "cers:  0.19230769230769232\n",
      "cers:  0.375\n",
      "cers:  0.14634146341463414\n",
      "cers:  0.2631578947368421\n",
      "cers:  0.37037037037037035\n",
      "=> loading checkpoint 'checkpoints/final_epoch_61.model'\n",
      "=> loaded checkpoint 'checkpoints/final_epoch_61.model' (epoch 60)\n",
      "cers:  0.10344827586206896\n",
      "cers:  0.2631578947368421\n",
      "cers:  0.125\n",
      "cers:  0.2619047619047619\n",
      "cers:  0.47619047619047616\n",
      "cers:  0.41379310344827586\n",
      "cers:  0.19230769230769232\n",
      "=> loading checkpoint 'checkpoints/final_epoch_61.model'\n",
      "=> loaded checkpoint 'checkpoints/final_epoch_61.model' (epoch 60)\n",
      "cers:  0.14634146341463414\n",
      "cers:  0.19230769230769232\n",
      "cers:  0.375\n",
      "cers:  0.3333333333333333\n",
      "cers:  0.2857142857142857\n",
      "cers:  0.41379310344827586\n",
      "cers:  0.2222222222222222\n",
      "=> loading checkpoint 'checkpoints/final_epoch_61.model'\n",
      "=> loaded checkpoint 'checkpoints/final_epoch_61.model' (epoch 60)\n",
      "cers:  0.125\n",
      "cers:  0.2916666666666667\n",
      "cers:  0.13636363636363635\n",
      "cers:  0.19230769230769232\n",
      "cers:  0.1794871794871795\n",
      "cers:  0.30952380952380953\n",
      "cers:  0.2631578947368421\n",
      "=> loading checkpoint 'checkpoints/final_epoch_61.model'\n",
      "=> loaded checkpoint 'checkpoints/final_epoch_61.model' (epoch 60)\n",
      "cers:  0.21052631578947367\n",
      "cers:  0.2222222222222222\n",
      "cers:  0.41379310344827586\n",
      "cers:  0.23076923076923078\n",
      "cers:  0.12195121951219512\n",
      "cers:  0.1\n",
      "cers:  0.2545454545454545\n",
      "=> loading checkpoint 'checkpoints/final_epoch_61.model'\n",
      "=> loaded checkpoint 'checkpoints/final_epoch_61.model' (epoch 60)\n",
      "cers:  0.19230769230769232\n",
      "cers:  0.48\n",
      "cers:  0.18518518518518517\n",
      "cers:  0.3181818181818182\n",
      "cers:  0.15384615384615385\n",
      "cers:  0.15\n",
      "cers:  0.23076923076923078\n",
      "=> loading checkpoint 'checkpoints/final_epoch_61.model'\n",
      "=> loaded checkpoint 'checkpoints/final_epoch_61.model' (epoch 60)\n",
      "cers:  0.10344827586206896\n",
      "cers:  0.2631578947368421\n",
      "cers:  0.19230769230769232\n",
      "cers:  0.35714285714285715\n",
      "cers:  0.2857142857142857\n",
      "cers:  0.3793103448275862\n",
      "cers:  0.375\n",
      "=> loading checkpoint 'checkpoints/final_epoch_61.model'\n",
      "=> loaded checkpoint 'checkpoints/final_epoch_61.model' (epoch 60)\n",
      "cers:  0.1282051282051282\n",
      "cers:  0.42857142857142855\n",
      "cers:  0.2545454545454545\n",
      "cers:  0.2631578947368421\n",
      "cers:  0.41379310344827586\n",
      "cers:  0.44\n",
      "cers:  0.3389830508474576\n",
      "=> loading checkpoint 'checkpoints/final_epoch_61.model'\n",
      "=> loaded checkpoint 'checkpoints/final_epoch_61.model' (epoch 60)\n",
      "cers:  0.41379310344827586\n",
      "cers:  0.3728813559322034\n",
      "cers:  0.25\n",
      "cers:  0.19230769230769232\n",
      "cers:  0.25\n",
      "cers:  0.42857142857142855\n",
      "cers:  0.18518518518518517\n"
     ]
    }
   ],
   "source": [
    "#cers = []\n",
    "#cers_2 = []\n",
    "cers= []\n",
    "for epoch in range(1, 10):\n",
    "    model.eval()\n",
    "    training_loss, train_acc = 0, 0\n",
    "    eer, total_eer = 0, 0\n",
    "    test_loss=0\n",
    "    acc = []\n",
    "    ps = ['final_epoch_61.model', 'final_epoch_client161.model', 'final_epoch_client261.model']\n",
    "    model, optimizer, epoch_num = load_checkpoint(model, optimizer, \"checkpoints/final_epoch_61.model\")\n",
    "    #writer = SummaryWriter('testing_logs_2/{}'.format(p))\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, _data in enumerate(test_loader):\n",
    "            inputs, labels, input_lengths, label_lengths = _data \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # output = model(inputs, input_lengths)  # (batch, time, n_class)\n",
    "            output=model(inputs)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            test_loss += loss.item() / len(test_loader)\n",
    "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "            decoded_preds, decoded_targets = list(map(str.strip, decoded_preds)), list(map(str.strip, decoded_targets))\n",
    "            for j in range(len(decoded_preds)):\n",
    "                s = SequenceMatcher(None, decoded_targets[j], decoded_preds[j])\n",
    "                acc.append(s.ratio())\n",
    "                w = wer(decoded_targets[j], decoded_preds[j])\n",
    "                cers.append(w)\n",
    "            avg_acc = sum(acc)/len(acc)\n",
    "            # writer.add_scalar(\"test_accuracy\", avg_acc, iter_meter.get())\n",
    "            # writer.add_scalar('test_loss', test_loss, iter_meter.get())\n",
    "\n",
    "            print(\"cers: \", w)\n",
    "\n",
    "            # writer.add_scalar(\"WERs/\", wer(decoded_targets[j], decoded_preds[j]), iter_meter.get())\n",
    "            #print(\"Test Accuracy: {}, Test loss: {}\".format(avg_acc, test_loss))\n",
    "            iter_meter.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2813165873868443"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 450 samples = 0.2669897415699739, 0.2713728649789243\n",
    "\n",
    "sum(cers)/len(cers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([602, 4, 4])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 55])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 3., 3., 3., 3., 2., 2., 2., 3., 3., 3., 1., 3., 3., 3., 3., 3.,\n",
       "         3., 3., 3., 3., 3., 3., 3., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1.],\n",
       "        [1., 1., 1., 1., 3., 3., 3., 3., 3., 3., 3., 3., 2., 2., 3., 3., 3., 3.,\n",
       "         3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1.],\n",
       "        [1., 1., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 2., 2., 2., 2., 3., 3.,\n",
       "         3., 3., 3., 3., 2., 2., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "         3., 1., 1., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "         1.],\n",
       "        [1., 2., 2., 2., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "         3., 3., 3., 3., 3., 3., 3., 3., 3., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1.]], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.25925925925925924,\n",
       "  0.13636363636363635,\n",
       "  0.3076923076923077,\n",
       "  0.48,\n",
       "  0.15789473684210525,\n",
       "  0.3793103448275862,\n",
       "  0.2631578947368421,\n",
       "  0.125,\n",
       "  0.38095238095238093,\n",
       "  0.23333333333333334,\n",
       "  0.07317073170731707,\n",
       "  0.18518518518518517,\n",
       "  0.35714285714285715,\n",
       "  0.4,\n",
       "  0.2413793103448276,\n",
       "  0.375,\n",
       "  0.14285714285714285,\n",
       "  0.3448275862068966,\n",
       "  0.2,\n",
       "  0.20512820512820512,\n",
       "  0.47619047619047616,\n",
       "  0.07692307692307693,\n",
       "  0.3050847457627119,\n",
       "  0.2,\n",
       "  0.25,\n",
       "  0.3076923076923077,\n",
       "  0.2222222222222222,\n",
       "  0.20689655172413793,\n",
       "  0.11904761904761904,\n",
       "  0.25,\n",
       "  0.3,\n",
       "  0.375,\n",
       "  0.21052631578947367,\n",
       "  0.20689655172413793,\n",
       "  0.14814814814814814,\n",
       "  0.38095238095238093,\n",
       "  0.3076923076923077,\n",
       "  0.13636363636363635,\n",
       "  0.3076923076923077,\n",
       "  0.2727272727272727,\n",
       "  0.18518518518518517,\n",
       "  0.25925925925925924,\n",
       "  0.41379310344827586,\n",
       "  0.35714285714285715,\n",
       "  0.11538461538461539,\n",
       "  0.23076923076923078,\n",
       "  0.48,\n",
       "  0.3103448275862069,\n",
       "  0.288135593220339,\n",
       "  0.47619047619047616,\n",
       "  0.23333333333333334,\n",
       "  0.07317073170731707,\n",
       "  0.3448275862068966,\n",
       "  0.4,\n",
       "  0.125,\n",
       "  0.2,\n",
       "  0.3448275862068966,\n",
       "  0.15789473684210525,\n",
       "  0.11904761904761904,\n",
       "  0.38095238095238093,\n",
       "  0.3103448275862069,\n",
       "  0.48,\n",
       "  0.3076923076923077,\n",
       "  0.11538461538461539,\n",
       "  0.3,\n",
       "  0.20689655172413793,\n",
       "  0.4375,\n",
       "  0.2727272727272727,\n",
       "  0.3076923076923077,\n",
       "  0.2222222222222222,\n",
       "  0.07317073170731707,\n",
       "  0.35714285714285715,\n",
       "  0.2,\n",
       "  0.25,\n",
       "  0.2222222222222222,\n",
       "  0.3050847457627119,\n",
       "  0.14814814814814814,\n",
       "  0.125,\n",
       "  0.13636363636363635,\n",
       "  0.47619047619047616,\n",
       "  0.20512820512820512,\n",
       "  0.2,\n",
       "  0.4,\n",
       "  0.2631578947368421,\n",
       "  0.47619047619047616,\n",
       "  0.13636363636363635,\n",
       "  0.35714285714285715,\n",
       "  0.21818181818181817,\n",
       "  0.38095238095238093,\n",
       "  0.3103448275862069,\n",
       "  0.25,\n",
       "  0.3181818181818182,\n",
       "  0.48,\n",
       "  0.23333333333333334,\n",
       "  0.4375,\n",
       "  0.2222222222222222,\n",
       "  0.288135593220339,\n",
       "  0.07317073170731707,\n",
       "  0.3448275862068966,\n",
       "  0.3076923076923077,\n",
       "  0.3076923076923077,\n",
       "  0.125,\n",
       "  0.26666666666666666,\n",
       "  0.23076923076923078,\n",
       "  0.2631578947368421,\n",
       "  0.2222222222222222,\n",
       "  0.25925925925925924,\n",
       "  0.11538461538461539,\n",
       "  0.4,\n",
       "  0.20689655172413793,\n",
       "  0.15789473684210525,\n",
       "  0.14285714285714285,\n",
       "  0.38095238095238093,\n",
       "  0.18518518518518517,\n",
       "  0.23636363636363636,\n",
       "  0.3220338983050847,\n",
       "  0.07317073170731707,\n",
       "  0.35714285714285715,\n",
       "  0.2413793103448276,\n",
       "  0.13636363636363635,\n",
       "  0.3,\n",
       "  0.3076923076923077,\n",
       "  0.25925925925925924,\n",
       "  0.09523809523809523,\n",
       "  0.3448275862068966,\n",
       "  0.25,\n",
       "  0.15789473684210525,\n",
       "  0.2564102564102564,\n",
       "  0.3793103448275862,\n",
       "  0.20689655172413793,\n",
       "  0.2,\n",
       "  0.425,\n",
       "  0.3076923076923077,\n",
       "  0.3181818181818182,\n",
       "  0.25925925925925924,\n",
       "  0.1,\n",
       "  0.47619047619047616,\n",
       "  0.07692307692307693,\n",
       "  0.2631578947368421,\n",
       "  0.4375,\n",
       "  0.2222222222222222,\n",
       "  0.38095238095238093,\n",
       "  0.23333333333333334,\n",
       "  0.2727272727272727,\n",
       "  0.13636363636363635,\n",
       "  0.07692307692307693,\n",
       "  0.47619047619047616,\n",
       "  0.3076923076923077,\n",
       "  0.4375,\n",
       "  0.20512820512820512,\n",
       "  0.0975609756097561,\n",
       "  0.3389830508474576,\n",
       "  0.15,\n",
       "  0.25925925925925924,\n",
       "  0.3448275862068966,\n",
       "  0.425,\n",
       "  0.48,\n",
       "  0.25,\n",
       "  0.21052631578947367,\n",
       "  0.3076923076923077,\n",
       "  0.14814814814814814,\n",
       "  0.27586206896551724,\n",
       "  0.2,\n",
       "  0.26666666666666666,\n",
       "  0.3793103448275862,\n",
       "  0.09523809523809523,\n",
       "  0.35714285714285715,\n",
       "  0.1724137931034483,\n",
       "  0.3076923076923077,\n",
       "  0.2727272727272727,\n",
       "  0.47619047619047616,\n",
       "  0.20689655172413793,\n",
       "  0.3448275862068966,\n",
       "  0.23333333333333334,\n",
       "  0.41379310344827586,\n",
       "  0.3076923076923077,\n",
       "  0.2222222222222222,\n",
       "  0.2,\n",
       "  0.4375,\n",
       "  0.35714285714285715,\n",
       "  0.48,\n",
       "  0.07317073170731707,\n",
       "  0.2631578947368421,\n",
       "  0.25,\n",
       "  0.15789473684210525,\n",
       "  0.42857142857142855,\n",
       "  0.425,\n",
       "  0.3050847457627119,\n",
       "  0.1,\n",
       "  0.11538461538461539,\n",
       "  0.2564102564102564,\n",
       "  0.26666666666666666,\n",
       "  0.18518518518518517,\n",
       "  0.13793103448275862,\n",
       "  0.14814814814814814,\n",
       "  0.11904761904761904,\n",
       "  0.25,\n",
       "  0.2631578947368421,\n",
       "  0.3793103448275862,\n",
       "  0.2,\n",
       "  0.38095238095238093,\n",
       "  0.10344827586206896,\n",
       "  0.1,\n",
       "  0.3076923076923077,\n",
       "  0.11904761904761904,\n",
       "  0.4,\n",
       "  0.2727272727272727,\n",
       "  0.3,\n",
       "  0.3103448275862069,\n",
       "  0.35714285714285715,\n",
       "  0.07317073170731707,\n",
       "  0.48,\n",
       "  0.13636363636363635,\n",
       "  0.2564102564102564,\n",
       "  0.2222222222222222,\n",
       "  0.4375,\n",
       "  0.288135593220339,\n",
       "  0.18518518518518517,\n",
       "  0.47619047619047616,\n",
       "  0.3448275862068966,\n",
       "  0.3076923076923077,\n",
       "  0.2222222222222222,\n",
       "  0.15789473684210525,\n",
       "  0.16666666666666666,\n",
       "  0.23333333333333334,\n",
       "  0.16666666666666666,\n",
       "  0.3050847457627119,\n",
       "  0.2222222222222222,\n",
       "  0.2,\n",
       "  0.48,\n",
       "  0.3448275862068966,\n",
       "  0.14814814814814814,\n",
       "  0.13636363636363635,\n",
       "  0.3076923076923077,\n",
       "  0.3125,\n",
       "  0.3076923076923077,\n",
       "  0.2564102564102564,\n",
       "  0.2727272727272727,\n",
       "  0.15789473684210525,\n",
       "  0.11538461538461539,\n",
       "  0.07317073170731707,\n",
       "  0.26666666666666666,\n",
       "  0.2631578947368421,\n",
       "  0.425,\n",
       "  0.1724137931034483,\n",
       "  0.2413793103448276,\n",
       "  0.15,\n",
       "  0.3793103448275862,\n",
       "  0.35714285714285715,\n",
       "  0.47619047619047616,\n",
       "  0.25,\n",
       "  0.2222222222222222],\n",
       " [0.2727272727272727,\n",
       "  0.3793103448275862,\n",
       "  0.2413793103448276,\n",
       "  0.47619047619047616,\n",
       "  0.2222222222222222,\n",
       "  0.42857142857142855,\n",
       "  0.2631578947368421,\n",
       "  0.16666666666666666,\n",
       "  0.27586206896551724,\n",
       "  0.35,\n",
       "  0.42857142857142855,\n",
       "  0.2711864406779661,\n",
       "  0.3076923076923077,\n",
       "  0.20512820512820512,\n",
       "  0.2,\n",
       "  0.26666666666666666,\n",
       "  0.21052631578947367,\n",
       "  0.13636363636363635,\n",
       "  0.375,\n",
       "  0.25,\n",
       "  0.15384615384615385,\n",
       "  0.25925925925925924,\n",
       "  0.18518518518518517,\n",
       "  0.2413793103448276,\n",
       "  0.2,\n",
       "  0.175,\n",
       "  0.48,\n",
       "  0.14634146341463414,\n",
       "  0.20512820512820512,\n",
       "  0.2727272727272727,\n",
       "  0.0975609756097561,\n",
       "  0.25925925925925924,\n",
       "  0.2711864406779661,\n",
       "  0.48,\n",
       "  0.2631578947368421,\n",
       "  0.42857142857142855,\n",
       "  0.47619047619047616,\n",
       "  0.25,\n",
       "  0.3076923076923077,\n",
       "  0.41379310344827586,\n",
       "  0.15384615384615385,\n",
       "  0.13636363636363635,\n",
       "  0.20689655172413793,\n",
       "  0.16666666666666666,\n",
       "  0.23333333333333334,\n",
       "  0.375,\n",
       "  0.2,\n",
       "  0.2413793103448276,\n",
       "  0.25925925925925924,\n",
       "  0.23076923076923078,\n",
       "  0.42857142857142855,\n",
       "  0.2631578947368421,\n",
       "  0.18518518518518517,\n",
       "  0.375,\n",
       "  0.27586206896551724,\n",
       "  0.175,\n",
       "  0.3793103448275862,\n",
       "  0.42857142857142855,\n",
       "  0.18518518518518517,\n",
       "  0.21052631578947367,\n",
       "  0.125,\n",
       "  0.47619047619047616,\n",
       "  0.2,\n",
       "  0.2692307692307692,\n",
       "  0.42857142857142855,\n",
       "  0.35,\n",
       "  0.14814814814814814,\n",
       "  0.3076923076923077,\n",
       "  0.23333333333333334,\n",
       "  0.2,\n",
       "  0.2727272727272727,\n",
       "  0.2631578947368421,\n",
       "  0.13636363636363635,\n",
       "  0.25,\n",
       "  0.25925925925925924,\n",
       "  0.15384615384615385,\n",
       "  0.1724137931034483,\n",
       "  0.375,\n",
       "  0.12195121951219512,\n",
       "  0.48,\n",
       "  0.2413793103448276,\n",
       "  0.20512820512820512,\n",
       "  0.2711864406779661,\n",
       "  0.16666666666666666,\n",
       "  0.15384615384615385,\n",
       "  0.27586206896551724,\n",
       "  0.2,\n",
       "  0.2413793103448276,\n",
       "  0.21818181818181817,\n",
       "  0.23333333333333334,\n",
       "  0.3076923076923077,\n",
       "  0.375,\n",
       "  0.20512820512820512,\n",
       "  0.42857142857142855,\n",
       "  0.14285714285714285,\n",
       "  0.2413793103448276,\n",
       "  0.42857142857142855,\n",
       "  0.47619047619047616,\n",
       "  0.13636363636363635,\n",
       "  0.48,\n",
       "  0.175,\n",
       "  0.15789473684210525,\n",
       "  0.14634146341463414,\n",
       "  0.375,\n",
       "  0.2222222222222222,\n",
       "  0.25925925925925924,\n",
       "  0.3793103448275862,\n",
       "  0.2711864406779661,\n",
       "  0.2631578947368421,\n",
       "  0.18518518518518517,\n",
       "  0.23076923076923078,\n",
       "  0.2727272727272727,\n",
       "  0.42857142857142855,\n",
       "  0.2631578947368421,\n",
       "  0.27586206896551724,\n",
       "  0.20512820512820512,\n",
       "  0.375,\n",
       "  0.47619047619047616,\n",
       "  0.42857142857142855,\n",
       "  0.3076923076923077,\n",
       "  0.25925925925925924,\n",
       "  0.2631578947368421,\n",
       "  0.2542372881355932,\n",
       "  0.41379310344827586,\n",
       "  0.13636363636363635,\n",
       "  0.15384615384615385,\n",
       "  0.23076923076923078,\n",
       "  0.2413793103448276,\n",
       "  0.2,\n",
       "  0.2727272727272727,\n",
       "  0.175,\n",
       "  0.2413793103448276,\n",
       "  0.21818181818181817,\n",
       "  0.11904761904761904,\n",
       "  0.2222222222222222,\n",
       "  0.1951219512195122,\n",
       "  0.2222222222222222,\n",
       "  0.25,\n",
       "  0.375,\n",
       "  0.26666666666666666,\n",
       "  0.375,\n",
       "  0.175,\n",
       "  0.2962962962962963,\n",
       "  0.15384615384615385,\n",
       "  0.2413793103448276,\n",
       "  0.2,\n",
       "  0.2222222222222222,\n",
       "  0.16666666666666666,\n",
       "  0.2222222222222222,\n",
       "  0.2711864406779661,\n",
       "  0.2631578947368421,\n",
       "  0.13636363636363635,\n",
       "  0.25,\n",
       "  0.26666666666666666,\n",
       "  0.27586206896551724,\n",
       "  0.2413793103448276,\n",
       "  0.20512820512820512,\n",
       "  0.3076923076923077,\n",
       "  0.3181818181818182,\n",
       "  0.14634146341463414,\n",
       "  0.2692307692307692,\n",
       "  0.42857142857142855,\n",
       "  0.4523809523809524,\n",
       "  0.15789473684210525,\n",
       "  0.3,\n",
       "  0.42857142857142855,\n",
       "  0.375,\n",
       "  0.48,\n",
       "  0.2631578947368421,\n",
       "  0.375,\n",
       "  0.2711864406779661,\n",
       "  0.23076923076923078,\n",
       "  0.2413793103448276,\n",
       "  0.0975609756097561,\n",
       "  0.375,\n",
       "  0.25925925925925924,\n",
       "  0.2727272727272727,\n",
       "  0.42857142857142855,\n",
       "  0.2692307692307692,\n",
       "  0.48,\n",
       "  0.21818181818181817,\n",
       "  0.3076923076923077,\n",
       "  0.15,\n",
       "  0.2631578947368421,\n",
       "  0.3793103448275862,\n",
       "  0.26666666666666666,\n",
       "  0.25,\n",
       "  0.3103448275862069,\n",
       "  0.2222222222222222,\n",
       "  0.26666666666666666,\n",
       "  0.42857142857142855,\n",
       "  0.14285714285714285,\n",
       "  0.15384615384615385,\n",
       "  0.2222222222222222,\n",
       "  0.27586206896551724,\n",
       "  0.13636363636363635,\n",
       "  0.3793103448275862,\n",
       "  0.42857142857142855,\n",
       "  0.175,\n",
       "  0.21052631578947367,\n",
       "  0.2631578947368421,\n",
       "  0.25925925925925924,\n",
       "  0.19047619047619047,\n",
       "  0.13636363636363635,\n",
       "  0.3076923076923077,\n",
       "  0.2413793103448276,\n",
       "  0.2413793103448276,\n",
       "  0.2,\n",
       "  0.25,\n",
       "  0.375,\n",
       "  0.48,\n",
       "  0.23076923076923078,\n",
       "  0.27586206896551724,\n",
       "  0.3181818181818182,\n",
       "  0.2222222222222222,\n",
       "  0.375,\n",
       "  0.2222222222222222,\n",
       "  0.15384615384615385,\n",
       "  0.3,\n",
       "  0.2,\n",
       "  0.12195121951219512,\n",
       "  0.47619047619047616,\n",
       "  0.2711864406779661,\n",
       "  0.42857142857142855,\n",
       "  0.3793103448275862,\n",
       "  0.2631578947368421,\n",
       "  0.2,\n",
       "  0.48,\n",
       "  0.25925925925925924,\n",
       "  0.3076923076923077,\n",
       "  0.20689655172413793,\n",
       "  0.14634146341463414,\n",
       "  0.15384615384615385,\n",
       "  0.42857142857142855,\n",
       "  0.2711864406779661,\n",
       "  0.13636363636363635,\n",
       "  0.21818181818181817,\n",
       "  0.27586206896551724,\n",
       "  0.21052631578947367,\n",
       "  0.42857142857142855,\n",
       "  0.2692307692307692,\n",
       "  0.25925925925925924,\n",
       "  0.175,\n",
       "  0.1794871794871795,\n",
       "  0.2727272727272727,\n",
       "  0.35,\n",
       "  0.25,\n",
       "  0.27586206896551724,\n",
       "  0.19047619047619047,\n",
       "  0.3,\n",
       "  0.375,\n",
       "  0.47619047619047616])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cers_1, cers_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16,10])\n",
    "plt.plot(gaussian_filter1d(cers, sigma=20), label = \"900 samples\")\n",
    "plt.plot(gaussian_filter1d(cers_1, sigma=20), label = \"450 samples (1)\")\n",
    "plt.plot(gaussian_filter1d(cers_2, sigma=20), label = \"450 samples (2)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MSIR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
