{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.47 s, sys: 468 ms, total: 2.93 s\n",
      "Wall time: 15 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dexter/Desktop/Projects/CodeSwitching/hparam.py:11: YAMLLoadWarning: calling yaml.load_all() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  for doc in docs:\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torchaudio\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from difflib import SequenceMatcher\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.functional import F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from librosa.core import stft, magphase\n",
    "from torch.autograd import Variable\n",
    "from data_load import CodeSwitchDataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeSwitchDataset(Dataset):\n",
    "    def __init__(self, lang, client, mode = \"train\", shuffle=True):\n",
    "        self.mode = mode\n",
    "        # data path\n",
    "        self.lang = lang\n",
    "        if self.lang == \"Gujarati\":\n",
    "            self.max_len = 0\n",
    "        elif self.lang == \"Telugu\":\n",
    "            self.max_len = 529862\n",
    "        elif self.lang == 'Tamil':\n",
    "            self.max_len = 0\n",
    "        else:\n",
    "            raise Exception(\"Check Language\")\n",
    "        if self.mode == \"train\":\n",
    "            self.path = 'Data/PartB_{}/Dev/'.format(self.lang)\n",
    "        elif self.mode == \"test\":\n",
    "            self.path = self.path = 'Data/PartB_{}/Dev/'.format(self.lang)\n",
    "        else:\n",
    "            raise Exception(\"Incorrect mode\")\n",
    "        self.file_list = os.listdir(os.path.join(self.path, 'Audio'))\n",
    "        self.shuffle=shuffle\n",
    "        self.csv_file = pd.read_csv(self.path + 'samples_450_{}.tsv'.format(client), header=None, sep='\\t')\n",
    "        self.input_length = []\n",
    "        self.label_length = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_file)\n",
    "\n",
    "    def pad(self, wav, trans, max_len):\n",
    "        orig_len = len(wav)\n",
    "        while len(wav) < max_len:\n",
    "            diff = max_len - len(wav)\n",
    "            ext = wav[:diff]\n",
    "            wav = np.append(wav, wav[:diff])\n",
    "            ratio = int(len(trans)*diff/len(wav))\n",
    "            trans +=trans[:ratio]\n",
    "        return wav, trans\n",
    "\n",
    "    def preprocess(self, wav, sr, trans):\n",
    "\n",
    "        out = stft(wav, win_length=int(sr*0.02), hop_length=int(sr*0.01))\n",
    "        text_transform = TextTransform()\n",
    "        trans = torch.Tensor(text_transform.text_to_int(trans.lower()))\n",
    "\n",
    "        out = magphase(out)[0]\n",
    "        out = [np.log(1 + x) for x in out]\n",
    "        return np.array(out), trans\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.csv_file[0][idx]\n",
    "        trans = self.csv_file[1][idx]\n",
    "        wav, sr = librosa.load(glob(self.path + 'Audio/*'+ str(file_name) + '.wav')[0])\n",
    "\n",
    "        if len(set(trans)) > 2:\n",
    "            label = 1\n",
    "        elif len(set(trans)) == 1 or len(set(trans)) == 2:\n",
    "            label = 0\n",
    "        else:\n",
    "            raise Exception(\"Check transcript\")\n",
    "        if self.mode ==\"train\":\n",
    "            return wav, sr, trans, self.lang\n",
    "        elif self.mode == \"test\":\n",
    "            return wav\n",
    "        else:\n",
    "            raise Exception(\"Incorrect Mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransform:\n",
    "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
    "    def __init__(self):\n",
    "        char_map_str = \"\"\"\n",
    "        s 1\n",
    "        e 2\n",
    "        t 3\n",
    "        \"\"\"\n",
    "        self.char_map = {}\n",
    "        self.index_map = {}\n",
    "        for line in char_map_str.strip().split('\\n'):\n",
    "            ch, index = line.split()\n",
    "            self.char_map[ch] = int(index)\n",
    "            self.index_map[int(index)] = ch\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
    "        int_sequence = []\n",
    "        for c in text:\n",
    "            if c == ' ':\n",
    "                ch = self.char_map['']\n",
    "            else:\n",
    "                ch = self.char_map[c]\n",
    "            int_sequence.append(ch)\n",
    "        return int_sequence\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
    "        string = []\n",
    "        for i in labels:\n",
    "            string.append(self.index_map[i])\n",
    "        return ''.join(string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_transforms = nn.Sequential(\n",
    "            torchaudio.transforms.MelSpectrogram(n_mels=128, sample_rate = 22050, n_fft = 512, win_length=int(22050*0.02), hop_length=int(22050*0.01)),\n",
    "            torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
    "            torchaudio.transforms.TimeMasking(time_mask_param=35)\n",
    "        )\n",
    "\n",
    "\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
    "\n",
    "text_transform = TextTransform()\n",
    "\n",
    "\n",
    "def data_processing(data, data_type=\"train\"):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    for (waveform, _, utterance, _) in data:\n",
    "        waveform=torch.Tensor(waveform)\n",
    "        if data_type == 'train':\n",
    "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        spectrograms.append(spec)\n",
    "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0]//2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, padding_value = 1, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "def wer(r, h):\n",
    "    \"\"\"\n",
    "    Calculation of WER with Levenshtein distance.\n",
    "\n",
    "    Works only for iterables up to 254 elements (uint8).\n",
    "    O(nm) time ans space complexity.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    r : list\n",
    "    h : list\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> wer(\"who is there\".split(), \"is there\".split())\n",
    "    1\n",
    "    >>> wer(\"who is there\".split(), \"\".split())\n",
    "    3\n",
    "    >>> wer(\"\".split(), \"who is there\".split())\n",
    "    3\n",
    "    \"\"\"\n",
    "    # initialisation\n",
    "\n",
    "    d = np.zeros((len(r) + 1) * (len(h) + 1), dtype=np.uint8)\n",
    "    d = d.reshape((len(r) + 1, len(h) + 1))\n",
    "    for i in range(len(r) + 1):\n",
    "        for j in range(len(h) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "    for i in range(1, len(r) + 1):\n",
    "        for j in range(1, len(h) + 1):\n",
    "            if r[i - 1] == h[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    return d[len(r)][len(h)]\n",
    "\n",
    "def GreedyDecoder(output, labels, label_lengths, blank_label=0, collapse_repeated=True):\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    targets = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank_label:\n",
    "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
    "                    continue\n",
    "                decode.append(index.item())\n",
    "        decodes.append(text_transform.int_to_text(decode))\n",
    "    return decodes, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
    "    def __init__(self, n_feats):\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x (batch, channel, feature, time)\n",
    "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
    "\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
    "        except with layer norm instead of batch norm\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # (batch, channel, feature, time)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        return x # (batch, channel, feature, time)\n",
    "\n",
    "\n",
    "class BidirectionalGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "\n",
    "        self.BiGRU = nn.GRU(\n",
    "            input_size=rnn_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.gelu(x)\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    \"\"\"Speech Recognition Model Inspired by DeepSpeech 2\"\"\"\n",
    "\n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        n_feats = n_feats//2\n",
    "        self.cnn = nn.Conv2d(1, 64, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
    "\n",
    "        # n residual cnn layers with filter size of 32\n",
    "        self.rescnn_layers = nn.Sequential(*[\n",
    "\n",
    "            ResidualCNN(64, 64, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "        self.fully_connected = nn.Linear(n_feats*64, rnn_dim)\n",
    "        self.birnn_layers = nn.Sequential(*[\n",
    "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
    "            for i in range(n_rnn_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn_layers(x)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "        x = x.transpose(1, 2) # (batch, time, feature)\n",
    "        x = self.fully_connected(x)\n",
    "        x = self.birnn_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, _data, criterion, epoch, writer, client_no):\n",
    "    model.eval()\n",
    "    training_loss, train_acc = 0, 0\n",
    "    eer, total_eer = 0, 0\n",
    "    test_loss=0\n",
    "    acc = []\n",
    "    with torch.no_grad():\n",
    "        inputs, labels, input_lengths, label_lengths = _data \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # output = model(inputs, input_lengths)  # (batch, time, n_class)\n",
    "        output=model(inputs)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "\n",
    "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "        test_loss += loss.item() / len(test_loader)\n",
    "        decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "        decoded_preds, decoded_targets = list(map(str.strip, decoded_preds)), list(map(str.strip, decoded_targets))\n",
    "        for j in range(len(decoded_preds)):\n",
    "            s = SequenceMatcher(None, decoded_targets[j], decoded_preds[j])\n",
    "            acc.append(s.ratio())\n",
    "\n",
    "        avg_acc = sum(acc)/len(acc)\n",
    "        writer.add_scalar(\"{}/test_accuracy\".format(client_no), avg_acc, epoch)\n",
    "        writer.add_scalar(\"{}/WER\".format(client_no), wer(decoded_targets[j], decoded_preds[j]), iter_meter.get())\n",
    "        writer.add_scalar('{}/test_loss'.format(client_no), test_loss, epoch)\n",
    "        print(\"Test Accuracy: {}, Test loss: {}\".format(avg_acc, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, filename='checkpoint.pth.tar'):\n",
    "    # Note: Input model & optimizer should be pre-defined.  This routine only updates their states.\n",
    "    start_epoch = 1\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "        checkpoint = torch.load(filename)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(filename, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "\n",
    "    return model, optimizer, start_epoch\n",
    "\n",
    "\n",
    "class IterMeter(object):\n",
    "    \"\"\"keeps track of total iterations\"\"\"\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.val += 1\n",
    "\n",
    "    def get(self):\n",
    "        return self.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "hparams = {\n",
    "        \"n_cnn_layers\": 4,\n",
    "        \"n_rnn_layers\": 5,\n",
    "        \"rnn_dim\": 512,\n",
    "        \"n_class\": 4,\n",
    "        \"n_feats\": 128,\n",
    "        \"stride\": 2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"batch_size\": 4,\n",
    "        \"epochs\": 60\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.frameworks.torch.fl.utils import federated_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-92fcda17df3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mshuffle_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrandom_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdataset_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_split\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdataset_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset_client1 = CodeSwitchDataset(lang='Telugu', client = 'a', mode=\"train\")\n",
    "train_dataset_client2 = CodeSwitchDataset(lang='Telugu', client = 'b', mode=\"train\")\n",
    "validation_split = 0.2\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "dataset_size = len(train_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "client1_train_loader = DataLoader(train_dataset_client1,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          drop_last=True,\n",
    "                          num_workers = 6,\n",
    "                          sampler = train_sampler,\n",
    "                          collate_fn = lambda x: data_processing(x, 'train'))\n",
    "\n",
    "\n",
    "client2_train_loader = DataLoader(train_dataset_client2,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          drop_last=True,\n",
    "                          num_workers=6,\n",
    "                          sampler=train_sampler,\n",
    "                          collate_fn=lambda x: data_processing(x, 'train'))\n",
    "\n",
    "\n",
    "\n",
    "test_loader = DataLoader(train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          drop_last=True,\n",
    "                          num_workers=6,\n",
    "                          sampler=valid_sampler,\n",
    "                          collate_fn=lambda x: data_processing(x, 'valid'))\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client1_model = SpeechRecognitionModel(\n",
    "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "        ).to(device)\n",
    "client1_optimizer = optim.Adam(client1_model.parameters(), hparams['learning_rate'])\n",
    "\n",
    "client2_model = SpeechRecognitionModel(\n",
    "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "        ).to(device)\n",
    "client2_optimizer = optim.Adam(client2_model.parameters(), hparams['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CTCLoss(blank=0, reduction='mean').to(device)\n",
    "epochs = 60\n",
    "epoch_num = 1\n",
    "optimizer = optim.Adam(model.parameters(), hparams['learning_rate'])\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "    max_lr=hparams['learning_rate'],\n",
    "    steps_per_epoch=int(len(train_loader)),\n",
    "    epochs=hparams['epochs'],\n",
    "    anneal_strategy='linear')\n",
    "\n",
    "\n",
    "#model, optimizer, epoch_num = load_checkpoint(model, optimizer, \"checkpoints/ckpt_epoch_30_batch_id_1645.pth\")\n",
    "\n",
    "print(epoch_num)\n",
    "writer = SummaryWriter('train_logs_450_a_avg/')\n",
    "iter_meter = IterMeter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, _data, criterion, optimizer, epoch, iter_meter, writer, scheduler, client_no):\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    total_loss=0\n",
    "    LR = 0\n",
    "    train_loss = 0\n",
    "\n",
    "    avg_acc = 0\n",
    "    acc = []\n",
    "    wers = []\n",
    "    #bi, wav, label = batch_idx, wav, label\n",
    "    for g in optimizer.param_groups:\n",
    "        LR=g['lr']\n",
    "    wav, labels, input_lengths, label_lengths = _data\n",
    "    wav, labels = wav.to(device), labels.to(device)\n",
    "    # input_lengths, label_lengths = torch.IntTensor(input_lengths), torch.IntTensor(label_lengths)\n",
    "    wav = wav.to(device)\n",
    "    # wav = wav.float()\n",
    "    labels = labels.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    # output = model(wav, input_lengths)   #(batch, time, n_class) [4, 911, 3]\n",
    "    output = model(wav)\n",
    "\n",
    "    output = F.log_softmax(output, dim=2)\n",
    "    output = output.transpose(0,1)\n",
    "\n",
    "    # print(labels, label_lengths)\n",
    "    loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    #print(loss)\n",
    "    total_loss+=loss\n",
    "\n",
    "    iter_meter.step()\n",
    "    decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "    decoded_preds, decoded_targets = list(map(str.strip, decoded_preds)), list(map(str.strip, decoded_targets))\n",
    "    print(decoded_preds, decoded_targets)\n",
    "    print(\"preds: \", \"\".join(decoded_preds))\n",
    "    for j in range(len(decoded_preds)):\n",
    "        s = SequenceMatcher(None, decoded_targets[j], decoded_preds[j])\n",
    "        wers.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "        acc.append(s.ratio())\n",
    "\n",
    "    avg_acc = sum(acc)/len(acc)\n",
    "    writer.add_scalar(\"{}/accuracy/train_accuracy\".format(client_no), avg_acc, epoch)\n",
    "    writer.add_scalar('{}/accuracy/train_loss'.format(client_no), loss.item(), iter_meter.get())\n",
    "    writer.add_scalar('{}/CTCLoss'.format(client_no), loss, epoch*len(train_loader)+1)\n",
    "    writer.add_scalar('{}/TLoss'.format(client_no), total_loss, epoch*len(train_loader)+1)\n",
    "    writer.add_scalar(\"{}/Learning Rate\".format(client_no), LR, epoch)\n",
    "\n",
    "    writer.add_scalar(\"WER\", wer(decoded_targets[j], decoded_preds[j]), iter_meter.get())\n",
    "    \"\"\"if batch_idx % 100 == 0 or batch_idx == data_len:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(wav), data_len,\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "        print(\"Train Accuracy: {}, Train loss: {}\".format(avg_acc, train_loss))\n",
    "    \"\"\"\n",
    "\n",
    "    #print(decoded_preds[0])\n",
    "    if (epoch+1)%2 == 0:\n",
    "        model.eval().cpu()\n",
    "        ckpt_model_filename = \"{}_ckpt_epoch_\".format(client_no) + str(epoch+1) + \"_450a.pth\"\n",
    "        ckpt_model_path = os.path.join(\"checkpoints_avg/\", ckpt_model_filename)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            }, ckpt_model_path)\n",
    "        model.to(device).train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    train(model, device, train_loader, criterion, optimizer, epoch, iter_meter, writer, scheduler)\n",
    "    test(model, device, test_loader, criterion, epoch, writer)\n",
    "\n",
    "model.eval().cpu()\n",
    "save_model_filename = \"final_epoch_client1\" + str(epoch + 1)  + \".model\"\n",
    "save_model_path = os.path.join(\"checkpoints\", save_model_filename)\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_model_path)\n",
    "\n",
    "print(\"\\nDone, trained model saved at\", save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = [client1_model, client2_model]\n",
    "optimizers = [client1_optimizer, client2_optimizer]\n",
    "train_loaders = [client1_train_loader, client2_train_loader]\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    _test_data = next(iter(test_loader))\n",
    "    for i in range(len(models)):\n",
    "        _train_data = next(iter(train_loaders[i]))\n",
    "        models[i].to(device)\n",
    "        models[i] = train(models[i], device, _train_data, criterion, optimizers[i], epoch, iter_meter, writer, scheduler, \"client_{}\".format(i))\n",
    "        test(models[i], device, _test_data, criterion, epoch, writer, \"client_{}\".format(i))\n",
    "    fed_model = federated_avg({'client1': models[0],\n",
    "                              'client2': models[1]})\n",
    "    test(fed_model, device, _test_data, criterion, epoch, writer, \"client_{}\".format(i))\n",
    "fed_model.eval().cpu()\n",
    "save_model_filename = \"final_epoch_client1\" + str(epoch + 1)  + \".model\"\n",
    "save_model_path = os.path.join(\"fed_checkpoints\", save_model_filename)\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': fed_model.state_dict(),\n",
    "            }, save_model_path)\n",
    "\n",
    "print(\"\\nDone, trained model saved at\", save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 128, 1177])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 128, 897])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.9348e-03, 6.9216e-03, 3.4567e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.6095e-02, 1.8771e-02, 9.3745e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [3.9755e-07, 1.5729e-15, 2.3203e-16,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.9780e-07, 1.3513e-15, 1.4307e-16,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.0308e-07, 1.2944e-15, 1.2784e-16,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]]],\n",
       " \n",
       " \n",
       "         [[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.6575e-02, 3.2842e-02, 1.2850e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.2070e-02, 8.9065e-02, 3.4849e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [3.5998e-06, 7.3058e-15, 3.3232e-16,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.6035e-06, 6.3003e-15, 3.8330e-16,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.6522e-06, 5.7338e-15, 1.7321e-15,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]]],\n",
       " \n",
       " \n",
       "         [[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.0525e-04, 1.3889e-03, 4.6921e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.5663e-04, 3.7667e-03, 1.2725e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [2.2641e-08, 3.2746e-16, 3.1243e-16,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.2781e-08, 2.9219e-16, 1.4813e-16,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.3159e-08, 2.5383e-16, 9.8389e-17,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]]],\n",
       " \n",
       " \n",
       "         [[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.1914e-06, 1.0250e-06, 7.1830e-08,  ..., 3.6329e-07,\n",
       "            1.3712e-06, 2.8189e-06],\n",
       "           [1.4079e-05, 2.7797e-06, 1.9480e-07,  ..., 9.8524e-07,\n",
       "            3.7187e-06, 7.6447e-06],\n",
       "           ...,\n",
       "           [2.3139e-07, 3.7314e-16, 4.7132e-16,  ..., 5.9783e-16,\n",
       "            3.6230e-16, 1.8816e-07],\n",
       "           [2.3074e-07, 3.1931e-16, 4.4585e-16,  ..., 4.8613e-16,\n",
       "            3.0082e-16, 1.8690e-07],\n",
       "           [2.3333e-07, 2.8152e-16, 4.1711e-16,  ..., 4.3068e-16,\n",
       "            2.4560e-16, 1.8854e-07]]]]),\n",
       " tensor([[1., 1., 1., 3., 3., 3., 3., 1., 2., 2., 2., 2., 2., 3., 3., 3., 3., 3.,\n",
       "          3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "          3., 3., 3., 3., 3., 3., 2., 2., 3., 3., 3., 3., 3., 3., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 3., 3., 2., 2., 2., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "          3., 3., 1., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 1., 1., 3., 3., 3.,\n",
       "          3., 3., 3., 3., 3., 3., 2., 2., 3., 3., 3., 3., 3., 1., 3., 3., 3., 3.,\n",
       "          3., 3., 3., 3., 3., 3., 3., 1.]]),\n",
       " [318, 328, 363, 441],\n",
       " [32, 33, 37, 44])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
