{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_lengths tensor([15,  8, 10], device='cuda:0') torch.Size([3])\n",
      "seq_tensor tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0') torch.Size([3, 15])\n",
      "seq_lengths torch.Size([3])\n",
      "packed length:  4\n",
      "packed length output:  4\n",
      "output:  torch.Size([15, 3, 5])\n",
      "final output:  tensor([[[-0.4176,  0.1017,  0.1668,  0.1087,  0.0277],\n",
      "         [-0.1694,  0.0083,  0.0499, -0.0340, -0.1278],\n",
      "         [-0.3136, -0.0012,  0.0833, -0.2994, -0.1930]],\n",
      "\n",
      "        [[-0.5143,  0.1591,  0.1161,  0.0153,  0.0206],\n",
      "         [ 0.0041, -0.0337, -0.2034, -0.3268, -0.5914],\n",
      "         [-0.3694, -0.0056, -0.0800,  0.0097,  0.0205]],\n",
      "\n",
      "        [[-0.2977,  0.0608, -0.0493, -0.1456, -0.2551],\n",
      "         [-0.1150, -0.0728, -0.2370, -0.5131, -0.3397],\n",
      "         [-0.3136,  0.0368, -0.1173, -0.3246, -0.3586]]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward>) torch.Size([3, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "seqs = ['gigantic_string','tiny_str','medium_str']\n",
    "\n",
    "# make <pad> idx 0\n",
    "vocab = ['<pad>'] + sorted(set(''.join(seqs)))\n",
    "\n",
    "# make model\n",
    "embed = nn.Embedding(len(vocab), 10).cuda()\n",
    "lstm = nn.LSTM(10, 5).cuda()\n",
    "\n",
    "vectorized_seqs = [[vocab.index(tok) for tok in seq] for seq in seqs]\n",
    "\n",
    "# get the length of each seq in your batch\n",
    "seq_lengths = torch.LongTensor([len(seq) for seq in vectorized_seqs]).cuda()\n",
    "print(\"seq_lengths\", seq_lengths, seq_lengths.shape)\n",
    "# dump padding everywhere, and place seqs on the left.\n",
    "# NOTE: you only need a tensor as big as your longest sequence\n",
    "seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long().cuda()\n",
    "print(\"seq_tensor\", seq_tensor, seq_tensor.shape)\n",
    "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "\tseq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "\n",
    "\n",
    "# SORT YOUR TENSORS BY LENGTH!\n",
    "seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "print(\"seq_lengths\", seq_lengths.shape)\n",
    "seq_tensor = seq_tensor[perm_idx]\n",
    "\n",
    "# utils.rnn lets you give (B,L,D) tensors where B is the batch size, L is the maxlength, if you use batch_first=True\n",
    "# Otherwise, give (L,B,D) tensors\n",
    "seq_tensor = seq_tensor.transpose(0,1) # (B,L,D) -> (L,B,D)\n",
    "\n",
    "# embed your sequences\n",
    "seq_tensor = embed(seq_tensor)\n",
    "\n",
    "# pack them up nicely\n",
    "packed_input = pack_padded_sequence(seq_tensor, seq_lengths.cpu().numpy())\n",
    "#print(\"packed_input: \", packed_input)\n",
    "print(\"packed length: \", len(packed_input))\n",
    "# throw them through your LSTM (remember to give batch_first=True here if you packed with it)\n",
    "packed_output, (ht, ct) = lstm(packed_input)\n",
    "print(\"packed length output: \", len(packed_output))\n",
    "# unpack your output if required\n",
    "output, _ = pad_packed_sequence(packed_output)\n",
    "print (\"output: \", output.shape)\n",
    "\n",
    "# Or if you just want the final hidden state?\n",
    "#print (\"final_hidden_state: \", ht[-1])\n",
    "\n",
    "# REMEMBER: Your outputs are sorted. If you want the original ordering\n",
    "# back (to compare to some gt labels) unsort them\n",
    "_, unperm_idx = perm_idx.sort(0)\n",
    "output = output[unperm_idx]\n",
    "print (\"final output: \", output, output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
